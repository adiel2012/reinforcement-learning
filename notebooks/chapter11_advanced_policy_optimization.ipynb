{"cells":[{"cell_type":"markdown","metadata":{"id":"JxlOjDpd1DfC"},"source":["# Chapter 11: Advanced Policy Optimization\n","\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/username/ReinforcementLearning/blob/main/notebooks/chapter11_advanced_policy_optimization.ipynb)\n","\n","## Introduction\n","\n","This chapter explores advanced policy optimization methods that build upon the basic policy gradient algorithms. We focus on trust region methods and proximal policy optimization, which address the key challenges of policy gradient methods: sample efficiency and stability.\n","\n","## References\n","- **Schulman et al. (2015)**: Trust Region Policy Optimization [11]\n","- **Schulman et al. (2017)**: Proximal Policy Optimization algorithms [12]\n","- **Sutton et al. (1999)**: Policy gradient methods for reinforcement learning [9]\n","- **Kakade (2001)**: Natural policy gradients and approximately optimal policies\n","\n","## Cross-References\n","- **Prerequisites**: Chapter 9 (Policy Gradients), Chapter 10 (Actor-Critic)\n","- **Next**: Chapter 12 (Model-Based Methods)\n","- **Related**: Chapter 8 (Deep RL), Chapter 15 (Meta-Learning)\n","\n","### Key Topics Covered:\n","- Trust Region Policy Optimization (TRPO)\n","- Proximal Policy Optimization (PPO)\n","- Natural Policy Gradients\n","- Importance Sampling and Policy Ratios\n","- Clipping Mechanisms and KL Divergence Constraints\n","\n","## Mathematical Foundation\n","\n","### Trust Region Methods\n","\n","The key insight behind trust region methods is to constrain policy updates to maintain a small KL divergence between the old and new policies:\n","\n","$$\\max_\\theta \\mathbb{E}_{s \\sim \\rho_{\\pi_{\\theta_{old}}}} \\left[ \\mathbb{E}_{a \\sim \\pi_{\\theta_{old}}(\\cdot|s)} \\left[ \\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta_{old}}(a|s)} A^{\\pi_{\\theta_{old}}}(s,a) \\right] \\right]$$\n","\n","subject to: $\\mathbb{E}_{s \\sim \\rho_{\\pi_{\\theta_{old}}}} [D_{KL}(\\pi_{\\theta_{old}}(\\cdot|s) \\| \\pi_\\theta(\\cdot|s))] \\leq \\delta$\n","\n","### Proximal Policy Optimization\n","\n","PPO simplifies TRPO by using a clipped objective function:\n","\n","$$L^{CLIP}(\\theta) = \\mathbb{E}_t \\left[ \\min(r_t(\\theta) A_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) A_t) \\right]$$\n","\n","where $r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}$ is the probability ratio."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jxqze5DT1DfI","executionInfo":{"status":"ok","timestamp":1754239673474,"user_tz":240,"elapsed":9887,"user":{"displayName":"Adiel Castano","userId":"12664058473489191316"}},"outputId":"2e2c2f24-ddb9-4497-a26a-150180bcf6d9"},"source":["# Import necessary libraries\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from collections import deque\n","import random\n","from typing import List, Tuple, Dict, Optional\n","\n","# Try to import PyTorch, fall back to NumPy implementation\n","try:\n","    import torch\n","    import torch.nn as nn\n","    import torch.optim as optim\n","    import torch.nn.functional as F\n","    from torch.distributions import Categorical, Normal\n","    HAS_TORCH = True\n","    print(\"PyTorch available - using neural network implementation\")\n","except ImportError:\n","    HAS_TORCH = False\n","    print(\"PyTorch not available - using NumPy implementation\")\n","\n","# Gym installation check\n","try:\n","    import gym\n","    HAS_GYM = True\n","except ImportError:\n","    HAS_GYM = False\n","    print(\"Gym not available - using custom environment\")\n","\n","# Set random seeds for reproducibility\n","np.random.seed(42)\n","random.seed(42)\n","if HAS_TORCH:\n","    torch.manual_seed(42)"],"outputs":[{"output_type":"stream","name":"stdout","text":["PyTorch available - using neural network implementation\n"]}]},{"cell_type":"markdown","metadata":{"id":"0ASV0bLt1DfM"},"source":["## Environment Setup\n","\n","We'll use a simple continuous control environment to demonstrate advanced policy optimization methods."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0it3ePgh1DfO","executionInfo":{"status":"ok","timestamp":1754239673600,"user_tz":240,"elapsed":74,"user":{"displayName":"Adiel Castano","userId":"12664058473489191316"}},"outputId":"5dcd1b09-b5ee-4a49-946e-a0ecc382ee29"},"source":["class ContinuousCartPole:\n","    \"\"\"Continuous version of CartPole for testing continuous control algorithms.\"\"\"\n","\n","    def __init__(self):\n","        self.gravity = 9.8\n","        self.masscart = 1.0\n","        self.masspole = 0.1\n","        self.total_mass = self.masspole + self.masscart\n","        self.length = 0.5\n","        self.polemass_length = self.masspole * self.length\n","        self.force_mag = 10.0\n","        self.tau = 0.02\n","\n","        # Thresholds\n","        self.theta_threshold_radians = 12 * 2 * np.pi / 360\n","        self.x_threshold = 2.4\n","\n","        self.action_space_high = np.array([1.0])\n","        self.action_space_low = np.array([-1.0])\n","        self.observation_space_high = np.array([4.8, np.inf, 0.4, np.inf])\n","        self.observation_space_low = -self.observation_space_high\n","\n","        self.state = None\n","        self.steps = 0\n","        self.max_steps = 500\n","\n","    def reset(self):\n","        self.state = np.random.uniform(low=-0.05, high=0.05, size=(4,))\n","        self.steps = 0\n","        return self.state.copy()\n","\n","    def step(self, action):\n","        action = np.clip(action, self.action_space_low, self.action_space_high)[0]\n","        force = action * self.force_mag\n","\n","        x, x_dot, theta, theta_dot = self.state\n","\n","        costheta = np.cos(theta)\n","        sintheta = np.sin(theta)\n","\n","        temp = (force + self.polemass_length * theta_dot ** 2 * sintheta) / self.total_mass\n","        thetaacc = (self.gravity * sintheta - costheta * temp) / (\n","            self.length * (4.0 / 3.0 - self.masspole * costheta ** 2 / self.total_mass)\n","        )\n","        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n","\n","        x = x + self.tau * x_dot\n","        x_dot = x_dot + self.tau * xacc\n","        theta = theta + self.tau * theta_dot\n","        theta_dot = theta_dot + self.tau * thetaacc\n","\n","        self.state = np.array([x, x_dot, theta, theta_dot])\n","        self.steps += 1\n","\n","        done = (\n","            x < -self.x_threshold or x > self.x_threshold or\n","            theta < -self.theta_threshold_radians or theta > self.theta_threshold_radians or\n","            self.steps >= self.max_steps\n","        )\n","\n","        reward = 1.0 if not done else 0.0\n","\n","        return self.state.copy(), reward, done, {}\n","\n","# Test the environment\n","env = ContinuousCartPole()\n","state = env.reset()\n","print(f\"Initial state: {state}\")\n","print(f\"State space: {len(state)} dimensions\")\n","print(f\"Action space: continuous [-1, 1]\")"],"outputs":[{"output_type":"stream","name":"stdout","text":["Initial state: [-0.01254599  0.04507143  0.02319939  0.00986585]\n","State space: 4 dimensions\n","Action space: continuous [-1, 1]\n"]}]},{"cell_type":"markdown","metadata":{"id":"h4dGG3Dv1DfQ"},"source":["## Trust Region Policy Optimization (TRPO)\n","\n","TRPO ensures that policy updates don't deviate too much from the current policy by constraining the KL divergence."]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IYkF_IRp1DfS","executionInfo":{"status":"ok","timestamp":1754239673717,"user_tz":240,"elapsed":110,"user":{"displayName":"Adiel Castano","userId":"12664058473489191316"}},"outputId":"eee4f89c-9efe-412e-c35d-7b2b979c6446"},"source":["if HAS_TORCH:\n","    class PolicyNetwork(nn.Module):\n","        def __init__(self, state_dim, action_dim, hidden_dim=64):\n","            super(PolicyNetwork, self).__init__()\n","            self.fc1 = nn.Linear(state_dim, hidden_dim)\n","            self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n","            self.mean = nn.Linear(hidden_dim, action_dim)\n","            self.log_std = nn.Parameter(torch.zeros(action_dim))\n","\n","        def forward(self, state):\n","            x = torch.relu(self.fc1(state))\n","            x = torch.relu(self.fc2(x))\n","            mean = self.mean(x)\n","            std = torch.exp(self.log_std)\n","            return Normal(mean, std)\n","\n","    class ValueNetwork(nn.Module):\n","        def __init__(self, state_dim, hidden_dim=64):\n","            super(ValueNetwork, self).__init__()\n","            self.fc1 = nn.Linear(state_dim, hidden_dim)\n","            self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n","            self.value = nn.Linear(hidden_dim, 1)\n","\n","        def forward(self, state):\n","            x = torch.relu(self.fc1(state))\n","            x = torch.relu(self.fc2(x))\n","            return self.value(x)\n","\n","class TRPO:\n","    def __init__(self, state_dim, action_dim, lr_v=1e-3, gamma=0.99,\n","                 max_kl=0.01, damping=0.1, max_backtracks=10):\n","        self.state_dim = state_dim\n","        self.action_dim = action_dim\n","        self.gamma = gamma\n","        self.max_kl = max_kl\n","        self.damping = damping\n","        self.max_backtracks = max_backtracks\n","\n","        if HAS_TORCH:\n","            self.policy = PolicyNetwork(state_dim, action_dim)\n","            self.value_net = ValueNetwork(state_dim)\n","            self.value_optimizer = optim.Adam(self.value_net.parameters(), lr=lr_v)\n","        else:\n","            # NumPy implementation with simplified policy\n","            self.policy_params = np.random.randn(state_dim + 1, action_dim) * 0.1\n","            self.value_params = np.random.randn(state_dim + 1) * 0.1\n","\n","    def get_action(self, state):\n","        if HAS_TORCH:\n","            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n","            dist = self.policy(state_tensor)\n","            action = dist.sample()\n","            log_prob = dist.log_prob(action)\n","            return action.numpy()[0], log_prob.detach().numpy()[0]\n","        else:\n","            # Simple linear policy with noise\n","            state_aug = np.append(state, 1.0)  # Add bias\n","            mean = np.dot(state_aug, self.policy_params)\n","            action = np.random.normal(mean, 0.5)\n","            log_prob = -0.5 * ((action - mean) ** 2 / 0.25 + np.log(2 * np.pi * 0.25))\n","            return action, log_prob\n","\n","    def compute_advantages(self, rewards, values, dones):\n","        advantages = []\n","        returns = []\n","\n","        R = 0\n","        for i in reversed(range(len(rewards))):\n","            R = rewards[i] + self.gamma * R * (1 - dones[i])\n","            returns.insert(0, R)\n","\n","        returns = np.array(returns)\n","        advantages = returns - values\n","\n","        # Normalize advantages\n","        advantages = (advantages - np.mean(advantages)) / (np.std(advantages) + 1e-8)\n","\n","        return advantages, returns\n","\n","    def update(self, states, actions, rewards, log_probs, dones):\n","        states = np.array(states)\n","        actions = np.array(actions)\n","        rewards = np.array(rewards)\n","        log_probs = np.array(log_probs)\n","        dones = np.array(dones)\n","\n","        if HAS_TORCH:\n","            # Convert to tensors\n","            states_tensor = torch.FloatTensor(states)\n","            actions_tensor = torch.FloatTensor(actions)\n","            old_log_probs = torch.FloatTensor(log_probs)\n","\n","            # Compute values and advantages\n","            values = self.value_net(states_tensor).squeeze().detach().numpy()\n","            advantages, returns = self.compute_advantages(rewards, values, dones)\n","\n","            advantages_tensor = torch.FloatTensor(advantages)\n","            returns_tensor = torch.FloatTensor(returns)\n","\n","            # Update value network\n","            for _ in range(5):\n","                value_loss = F.mse_loss(self.value_net(states_tensor).squeeze(), returns_tensor)\n","                self.value_optimizer.zero_grad()\n","                value_loss.backward()\n","                self.value_optimizer.step()\n","\n","            # TRPO update (simplified)\n","            dist = self.policy(states_tensor)\n","            new_log_probs = dist.log_prob(actions_tensor)\n","            ratio = torch.exp(new_log_probs - old_log_probs)\n","\n","            surrogate_loss = -(ratio * advantages_tensor).mean()\n","\n","            # Simplified natural gradient step\n","            policy_params = list(self.policy.parameters())\n","            grads = torch.autograd.grad(surrogate_loss, policy_params, create_graph=True)\n","\n","            # Apply update with line search (simplified)\n","            with torch.no_grad():\n","                for param, grad in zip(policy_params, grads):\n","                    param -= 0.001 * grad  # Small step size\n","\n","        else:\n","            # Simple NumPy update\n","            state_aug = np.column_stack([states, np.ones(len(states))])\n","            values = np.dot(state_aug, self.value_params)\n","            advantages, returns = self.compute_advantages(rewards, values, dones)\n","\n","            # Update value function\n","            value_grad = np.dot(state_aug.T, returns - values) / len(states)\n","            self.value_params += 0.01 * value_grad\n","\n","            # Simple policy update\n","            policy_grad = np.zeros_like(self.policy_params)\n","            for i, (s, a, adv) in enumerate(zip(states, actions, advantages)):\n","                s_aug = np.append(s, 1.0)\n","                policy_grad += np.outer(s_aug, adv * (a - np.dot(s_aug, self.policy_params)))\n","\n","            self.policy_params += 0.001 * policy_grad / len(states)\n","\n","print(\"TRPO implementation ready\")"],"outputs":[{"output_type":"stream","name":"stdout","text":["TRPO implementation ready\n"]}]},{"cell_type":"markdown","metadata":{"id":"QcNBNTz31DfV"},"source":["## Proximal Policy Optimization (PPO)\n","\n","PPO is a simpler alternative to TRPO that uses clipping to prevent large policy updates."]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8rjXPCnE1DfW","executionInfo":{"status":"ok","timestamp":1754239673775,"user_tz":240,"elapsed":49,"user":{"displayName":"Adiel Castano","userId":"12664058473489191316"}},"outputId":"341a9b23-928f-4288-ccad-e4c4bc6ac2d3"},"source":["class PPO:\n","    def __init__(self, state_dim, action_dim, lr=3e-4, gamma=0.99,\n","                 eps_clip=0.2, epochs=10, mini_batch_size=64):\n","        self.state_dim = state_dim\n","        self.action_dim = action_dim\n","        self.gamma = gamma\n","        self.eps_clip = eps_clip\n","        self.epochs = epochs\n","        self.mini_batch_size = mini_batch_size\n","\n","        if HAS_TORCH:\n","            self.policy = PolicyNetwork(state_dim, action_dim)\n","            self.value_net = ValueNetwork(state_dim)\n","            self.optimizer = optim.Adam(\n","                list(self.policy.parameters()) + list(self.value_net.parameters()),\n","                lr=lr\n","            )\n","        else:\n","            # NumPy implementation\n","            self.policy_params = np.random.randn(state_dim + 1, action_dim) * 0.1\n","            self.value_params = np.random.randn(state_dim + 1) * 0.1\n","\n","    def get_action(self, state):\n","        if HAS_TORCH:\n","            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n","            with torch.no_grad():\n","                dist = self.policy(state_tensor)\n","                action = dist.sample()\n","                log_prob = dist.log_prob(action)\n","                value = self.value_net(state_tensor)\n","            return action.numpy()[0], log_prob.numpy()[0], value.numpy()[0]\n","        else:\n","            # Simple linear policy\n","            state_aug = np.append(state, 1.0)\n","            mean = np.dot(state_aug, self.policy_params)\n","            action = np.random.normal(mean, 0.5)\n","            log_prob = -0.5 * ((action - mean) ** 2 / 0.25 + np.log(2 * np.pi * 0.25))\n","            value = np.dot(state_aug, self.value_params)\n","            return action, log_prob, value\n","\n","    def compute_gae(self, rewards, values, dones, next_value, lam=0.95):\n","        advantages = []\n","        gae = 0\n","\n","        values = values + [next_value]\n","\n","        for i in reversed(range(len(rewards))):\n","            delta = rewards[i] + self.gamma * values[i + 1] * (1 - dones[i]) - values[i]\n","            gae = delta + self.gamma * lam * (1 - dones[i]) * gae\n","            advantages.insert(0, gae)\n","\n","        returns = [adv + val for adv, val in zip(advantages, values[:-1])]\n","\n","        return advantages, returns\n","\n","    def update(self, states, actions, rewards, log_probs, values, dones):\n","        states = np.array(states)\n","        actions = np.array(actions)\n","        rewards = np.array(rewards)\n","        old_log_probs = np.array(log_probs)\n","        old_values = np.array(values)\n","        dones = np.array(dones)\n","\n","        # Get next value for GAE calculation\n","        if HAS_TORCH:\n","            with torch.no_grad():\n","                next_value = self.value_net(torch.FloatTensor(states[-1:]))[0].item()\n","        else:\n","            next_value = np.dot(np.append(states[-1], 1.0), self.value_params)\n","\n","        advantages, returns = self.compute_gae(rewards.tolist(), old_values.tolist(),\n","                                               dones.tolist(), next_value)\n","\n","        advantages = np.array(advantages)\n","        returns = np.array(returns)\n","\n","        # Normalize advantages\n","        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n","\n","        if HAS_TORCH:\n","            # Convert to tensors\n","            states_tensor = torch.FloatTensor(states)\n","            actions_tensor = torch.FloatTensor(actions)\n","            old_log_probs_tensor = torch.FloatTensor(old_log_probs)\n","            advantages_tensor = torch.FloatTensor(advantages)\n","            returns_tensor = torch.FloatTensor(returns)\n","\n","            # PPO update\n","            for _ in range(self.epochs):\n","                # Get current policy and value outputs\n","                dist = self.policy(states_tensor)\n","                new_log_probs = dist.log_prob(actions_tensor)\n","                entropy = dist.entropy().mean()\n","                values_pred = self.value_net(states_tensor).squeeze()\n","\n","                # Compute ratio and clipped objective\n","                ratio = torch.exp(new_log_probs - old_log_probs_tensor)\n","                surr1 = ratio * advantages_tensor\n","                surr2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * advantages_tensor\n","\n","                # PPO loss\n","                policy_loss = -torch.min(surr1, surr2).mean()\n","                value_loss = F.mse_loss(values_pred, returns_tensor)\n","                entropy_loss = -0.01 * entropy\n","\n","                total_loss = policy_loss + 0.5 * value_loss + entropy_loss\n","\n","                self.optimizer.zero_grad()\n","                total_loss.backward()\n","                torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 0.5)\n","                self.optimizer.step()\n","\n","        else:\n","            # Simple NumPy PPO update\n","            for _ in range(min(self.epochs, 3)):  # Fewer epochs for NumPy\n","                # Update value function\n","                state_aug = np.column_stack([states, np.ones(len(states))])\n","                values_pred = np.dot(state_aug, self.value_params)\n","                value_grad = np.dot(state_aug.T, returns - values_pred) / len(states)\n","                self.value_params += 0.01 * value_grad\n","\n","                # Policy update with clipping\n","                policy_grad = np.zeros_like(self.policy_params)\n","                for i, (s, a, adv, old_lp) in enumerate(zip(states, actions, advantages, old_log_probs)):\n","                    s_aug = np.append(s, 1.0)\n","                    mean = np.dot(s_aug, self.policy_params)\n","                    new_lp = -0.5 * ((a - mean) ** 2 / 0.25 + np.log(2 * np.pi * 0.25))\n","\n","                    ratio = np.exp(new_lp - old_lp)\n","                    clipped_ratio = np.clip(ratio, 1 - self.eps_clip, 1 + self.eps_clip)\n","\n","                    objective = min(ratio * adv, clipped_ratio * adv)\n","                    policy_grad += np.outer(s_aug, objective * (a - mean) / 0.25)\n","\n","                self.policy_params += 0.001 * policy_grad / len(states)\n","\n","print(\"PPO implementation ready\")"],"outputs":[{"output_type":"stream","name":"stdout","text":["PPO implementation ready\n"]}]},{"cell_type":"markdown","metadata":{"id":"HaCM5noG1DfZ"},"source":["## Training and Comparison\n","\n","Let's train both TRPO and PPO on our continuous CartPole environment and compare their performance."]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FJPwk4IK1Dfb","executionInfo":{"status":"ok","timestamp":1754239751869,"user_tz":240,"elapsed":41288,"user":{"displayName":"Adiel Castano","userId":"12664058473489191316"}},"outputId":"1bf9ca76-6b92-43d1-b0a7-8ae52959c879"},"source":["def train_agent(agent, env, episodes=1000, max_steps=500):\n","    \"\"\"Train an agent and return training statistics.\"\"\"\n","    episode_rewards = []\n","    moving_avg_rewards = []\n","\n","    for episode in range(episodes):\n","        states, actions, rewards, log_probs, values, dones = [], [], [], [], [], []\n","\n","        state = env.reset()\n","        episode_reward = 0\n","\n","        for step in range(max_steps):\n","            if hasattr(agent, 'get_action'):\n","                if isinstance(agent, PPO):\n","                    action, log_prob, value = agent.get_action(state)\n","                    values.append(value[0])\n","                else:  # TRPO\n","                    action, log_prob = agent.get_action(state)\n","                    values.append(0)  # Placeholder\n","\n","            states.append(state)\n","            actions.append(action)\n","            log_probs.append(log_prob)\n","\n","            next_state, reward, done, _ = env.step(action)\n","\n","            rewards.append(reward)\n","            dones.append(done)\n","            episode_reward += reward\n","\n","            state = next_state\n","\n","            if done:\n","                break\n","\n","        # Update agent\n","        if isinstance(agent, PPO):\n","            agent.update(states, actions, rewards, log_probs, values, dones)\n","        else:  # TRPO\n","            agent.update(states, actions, rewards, log_probs, dones)\n","\n","        episode_rewards.append(episode_reward)\n","\n","        # Compute moving average\n","        if len(episode_rewards) >= 100:\n","            moving_avg = np.mean(episode_rewards[-100:])\n","        else:\n","            moving_avg = np.mean(episode_rewards)\n","        moving_avg_rewards.append(moving_avg)\n","\n","        if episode % 100 == 0:\n","            print(f\"Episode {episode}, Average Reward: {moving_avg:.2f}\")\n","\n","    return episode_rewards, moving_avg_rewards\n","\n","# Create environment\n","env = ContinuousCartPole()\n","state_dim = 4\n","action_dim = 1\n","\n","print(\"Training PPO agent...\")\n","ppo_agent = PPO(state_dim, action_dim)\n","ppo_rewards, ppo_moving_avg = train_agent(ppo_agent, env, episodes=500)\n","\n","print(\"\\nTraining TRPO agent...\")\n","trpo_agent = TRPO(state_dim, action_dim)\n","trpo_rewards, trpo_moving_avg = train_agent(trpo_agent, env, episodes=500)"],"outputs":[{"output_type":"stream","name":"stdout","text":["Training PPO agent...\n","Episode 0, Average Reward: 20.00\n","Episode 100, Average Reward: 23.81\n","Episode 200, Average Reward: 21.26\n","Episode 300, Average Reward: 22.84\n","Episode 400, Average Reward: 22.11\n","\n","Training TRPO agent...\n","Episode 0, Average Reward: 19.00\n","Episode 100, Average Reward: 24.18\n","Episode 200, Average Reward: 22.62\n","Episode 300, Average Reward: 24.19\n","Episode 400, Average Reward: 22.05\n"]}]},{"cell_type":"markdown","metadata":{"id":"xt4UI60C1Dfd"},"source":["## Performance Analysis and Visualization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k7OPP2Eh1Dfe","executionInfo":{"status":"aborted","timestamp":1754239683116,"user_tz":240,"elapsed":19613,"user":{"displayName":"Adiel Castano","userId":"12664058473489191316"}}},"source":["# Plot training curves\n","plt.figure(figsize=(15, 10))\n","\n","# Episode rewards\n","plt.subplot(2, 2, 1)\n","plt.plot(ppo_rewards, alpha=0.3, color='blue', label='PPO Episodes')\n","plt.plot(ppo_moving_avg, color='blue', linewidth=2, label='PPO Moving Average')\n","plt.plot(trpo_rewards, alpha=0.3, color='red', label='TRPO Episodes')\n","plt.plot(trpo_moving_avg, color='red', linewidth=2, label='TRPO Moving Average')\n","plt.xlabel('Episode')\n","plt.ylabel('Reward')\n","plt.title('Training Performance: PPO vs TRPO')\n","plt.legend()\n","plt.grid(True, alpha=0.3)\n","\n","# Moving averages comparison\n","plt.subplot(2, 2, 2)\n","plt.plot(ppo_moving_avg, color='blue', linewidth=2, label='PPO')\n","plt.plot(trpo_moving_avg, color='red', linewidth=2, label='TRPO')\n","plt.xlabel('Episode')\n","plt.ylabel('Moving Average Reward')\n","plt.title('Learning Curves Comparison')\n","plt.legend()\n","plt.grid(True, alpha=0.3)\n","\n","# Final performance histogram\n","plt.subplot(2, 2, 3)\n","final_ppo = ppo_rewards[-100:] if len(ppo_rewards) >= 100 else ppo_rewards\n","final_trpo = trpo_rewards[-100:] if len(trpo_rewards) >= 100 else trpo_rewards\n","plt.hist(final_ppo, alpha=0.7, bins=20, label=f'PPO (μ={np.mean(final_ppo):.1f})', color='blue')\n","plt.hist(final_trpo, alpha=0.7, bins=20, label=f'TRPO (μ={np.mean(final_trpo):.1f})', color='red')\n","plt.xlabel('Episode Reward')\n","plt.ylabel('Frequency')\n","plt.title('Final Performance Distribution (Last 100 Episodes)')\n","plt.legend()\n","plt.grid(True, alpha=0.3)\n","\n","# Convergence analysis\n","plt.subplot(2, 2, 4)\n","window_size = 50\n","ppo_std = [np.std(ppo_rewards[max(0, i-window_size):i+1]) for i in range(len(ppo_rewards))]\n","trpo_std = [np.std(trpo_rewards[max(0, i-window_size):i+1]) for i in range(len(trpo_rewards))]\n","plt.plot(ppo_std, color='blue', label='PPO Std Dev')\n","plt.plot(trpo_std, color='red', label='TRPO Std Dev')\n","plt.xlabel('Episode')\n","plt.ylabel('Reward Standard Deviation')\n","plt.title('Training Stability (Rolling Std Dev)')\n","plt.legend()\n","plt.grid(True, alpha=0.3)\n","\n","plt.tight_layout()\n","plt.show()\n","\n","# Print summary statistics\n","print(\"\\n=== Performance Summary ===\")\n","print(f\"PPO Final Average: {np.mean(final_ppo):.2f} ± {np.std(final_ppo):.2f}\")\n","print(f\"TRPO Final Average: {np.mean(final_trpo):.2f} ± {np.std(final_trpo):.2f}\")\n","print(f\"PPO Max Reward: {max(ppo_rewards):.2f}\")\n","print(f\"TRPO Max Reward: {max(trpo_rewards):.2f}\")"],"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yUTfM7rw1Dfg"},"source":["## Advanced Policy Analysis\n","\n","Let's analyze the learned policies and understand how they differ."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-1qSrigi1Dfh","executionInfo":{"status":"aborted","timestamp":1754239683142,"user_tz":240,"elapsed":19637,"user":{"displayName":"Adiel Castano","userId":"12664058473489191316"}}},"source":["def analyze_policy_behavior(agent, env, num_episodes=10):\n","    \"\"\"Analyze the behavior of a trained policy.\"\"\"\n","    trajectories = []\n","    rewards = []\n","\n","    for ep in range(num_episodes):\n","        trajectory = []\n","        state = env.reset()\n","        episode_reward = 0\n","\n","        for step in range(500):\n","            if isinstance(agent, PPO):\n","                action, _, _ = agent.get_action(state)\n","            else:\n","                action, _ = agent.get_action(state)\n","\n","            trajectory.append({\n","                'state': state.copy(),\n","                'action': action,\n","                'step': step\n","            })\n","\n","            state, reward, done, _ = env.step(action)\n","            episode_reward += reward\n","\n","            if done:\n","                break\n","\n","        trajectories.append(trajectory)\n","        rewards.append(episode_reward)\n","\n","    return trajectories, rewards\n","\n","# Analyze both agents\n","print(\"Analyzing PPO policy...\")\n","ppo_trajectories, ppo_eval_rewards = analyze_policy_behavior(ppo_agent, env)\n","\n","print(\"Analyzing TRPO policy...\")\n","trpo_trajectories, trpo_eval_rewards = analyze_policy_behavior(trpo_agent, env)\n","\n","# Visualize policy behavior\n","plt.figure(figsize=(15, 10))\n","\n","# State trajectories\n","for i, (ppo_traj, trpo_traj) in enumerate(zip(ppo_trajectories[:3], trpo_trajectories[:3])):\n","    # Cart position\n","    plt.subplot(3, 4, i*4 + 1)\n","    ppo_positions = [step['state'][0] for step in ppo_traj]\n","    trpo_positions = [step['state'][0] for step in trpo_traj]\n","    plt.plot(ppo_positions, 'b-', label='PPO' if i == 0 else '')\n","    plt.plot(trpo_positions, 'r-', label='TRPO' if i == 0 else '')\n","    plt.ylabel('Cart Position')\n","    plt.title(f'Episode {i+1}')\n","    if i == 0:\n","        plt.legend()\n","    plt.grid(True, alpha=0.3)\n","\n","    # Pole angle\n","    plt.subplot(3, 4, i*4 + 2)\n","    ppo_angles = [step['state'][2] for step in ppo_traj]\n","    trpo_angles = [step['state'][2] for step in trpo_traj]\n","    plt.plot(ppo_angles, 'b-')\n","    plt.plot(trpo_angles, 'r-')\n","    plt.ylabel('Pole Angle')\n","    plt.grid(True, alpha=0.3)\n","\n","    # Actions\n","    plt.subplot(3, 4, i*4 + 3)\n","    ppo_actions = [step['action'][0] if hasattr(step['action'], '__iter__') else step['action'] for step in ppo_traj]\n","    trpo_actions = [step['action'][0] if hasattr(step['action'], '__iter__') else step['action'] for step in trpo_traj]\n","    plt.plot(ppo_actions, 'b-')\n","    plt.plot(trpo_actions, 'r-')\n","    plt.ylabel('Action')\n","    plt.grid(True, alpha=0.3)\n","\n","    # Phase plot (position vs angle)\n","    plt.subplot(3, 4, i*4 + 4)\n","    plt.plot(ppo_positions, ppo_angles, 'b-', alpha=0.7)\n","    plt.plot(trpo_positions, trpo_angles, 'r-', alpha=0.7)\n","    plt.xlabel('Cart Position')\n","    plt.ylabel('Pole Angle')\n","    plt.title('Phase Plot')\n","    plt.grid(True, alpha=0.3)\n","\n","plt.tight_layout()\n","plt.show()\n","\n","# Performance comparison\n","print(\"\\n=== Evaluation Results ===\")\n","print(f\"PPO Evaluation Reward: {np.mean(ppo_eval_rewards):.2f} ± {np.std(ppo_eval_rewards):.2f}\")\n","print(f\"TRPO Evaluation Reward: {np.mean(trpo_eval_rewards):.2f} ± {np.std(trpo_eval_rewards):.2f}\")\n","print(f\"PPO Episode Lengths: {[len(traj) for traj in ppo_trajectories]}\")\n","print(f\"TRPO Episode Lengths: {[len(traj) for traj in trpo_trajectories]}\")"],"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VlcPF0gC1Dfi"},"source":["## Key Insights and Theoretical Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"92ODarOv1Dfj","executionInfo":{"status":"aborted","timestamp":1754239683151,"user_tz":240,"elapsed":19643,"user":{"displayName":"Adiel Castano","userId":"12664058473489191316"}}},"source":["# Compute policy divergence analysis\n","def compute_policy_statistics(agent, states_sample):\n","    \"\"\"Compute statistics about the policy.\"\"\"\n","    actions = []\n","    entropies = []\n","\n","    for state in states_sample:\n","        if isinstance(agent, PPO):\n","            action, _, _ = agent.get_action(state)\n","        else:\n","            action, _ = agent.get_action(state)\n","\n","        actions.append(action)\n","\n","        # Estimate entropy (simplified)\n","        if HAS_TORCH:\n","            with torch.no_grad():\n","                state_tensor = torch.FloatTensor(state).unsqueeze(0)\n","                if hasattr(agent, 'policy'):\n","                    dist = agent.policy(state_tensor)\n","                    entropy = dist.entropy().item()\n","                    entropies.append(entropy)\n","\n","    return {\n","        'action_mean': np.mean(actions),\n","        'action_std': np.std(actions),\n","        'entropy_mean': np.mean(entropies) if entropies else 0\n","    }\n","\n","# Sample states from different regions\n","np.random.seed(42)\n","sample_states = []\n","for _ in range(100):\n","    state = np.random.uniform(-1, 1, 4)\n","    sample_states.append(state)\n","\n","ppo_stats = compute_policy_statistics(ppo_agent, sample_states)\n","trpo_stats = compute_policy_statistics(trpo_agent, sample_states)\n","\n","print(\"\\n=== Policy Statistics ===\")\n","print(\"PPO Policy:\")\n","for key, value in ppo_stats.items():\n","    print(f\"  {key}: {value:.4f}\")\n","\n","print(\"\\nTRPO Policy:\")\n","for key, value in trpo_stats.items():\n","    print(f\"  {key}: {value:.4f}\")\n","\n","# Theoretical insights\n","print(\"\\n=== Theoretical Insights ===\")\n","print(\"\"\"\n","1. **PPO vs TRPO Trade-offs**:\n","   - PPO uses clipping for simplicity vs TRPO's KL constraint\n","   - PPO is computationally more efficient\n","   - TRPO provides stronger theoretical guarantees\n","\n","2. **Policy Optimization Challenges**:\n","   - Large policy updates can cause performance collapse\n","   - Both methods address the exploration-exploitation trade-off\n","   - Trust regions prevent destructive policy changes\n","\n","3. **Practical Considerations**:\n","   - PPO is more widely adopted due to implementation simplicity\n","   - Hyperparameter sensitivity varies between methods\n","   - Sample efficiency depends on environment characteristics\n","\"\"\")"],"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FJJ4HTVR1Dfk"},"source":["## Summary and Educational Insights\n","\n","### Key Takeaways from Advanced Policy Optimization:\n","\n","1. **Trust Region Methods**: TRPO constrains policy updates using KL divergence to ensure stable learning, preventing the policy from changing too dramatically in a single update [11].\n","\n","2. **Proximal Policy Optimization**: PPO achieves similar stability to TRPO through a simpler clipping mechanism, making it more practical to implement and tune [12].\n","\n","3. **Importance Sampling**: Both methods use importance sampling to reuse data from the old policy, improving sample efficiency compared to vanilla policy gradients [9].\n","\n","4. **Practical Impact**: These methods have become the foundation for many state-of-the-art RL applications, from robotics to game playing.\n","\n","### Mathematical Foundations:\n","\n","- **Policy Gradient Theorem**: $\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(a|s) A^\\pi(s,a)]$ [9]\n","- **Trust Region Constraint**: $\\mathbb{E}[D_{KL}(\\pi_{old} \\| \\pi_{new})] \\leq \\delta$ [11]\n","- **PPO Clipping**: $\\min(r_t(\\theta) A_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) A_t)$ [12]\n","\n","### Engineering Insights:\n","\n","- **Generalized Advantage Estimation (GAE)** reduces variance in advantage estimates\n","- **Mini-batch updates** improve computational efficiency\n","- **Entropy regularization** maintains exploration during training\n","- **Gradient clipping** prevents destructive updates\n","\n","These advanced policy optimization methods represent a significant evolution in reinforcement learning, providing the stability and efficiency needed for complex real-world applications while maintaining theoretical soundness.\n","\n","### References:\n","- [9] Sutton, R. S., et al. (1999). Policy gradient methods for reinforcement learning with function approximation\n","- [11] Schulman, J., et al. (2015). Trust region policy optimization. *International conference on machine learning*\n","- [12] Schulman, J., et al. (2017). Proximal policy optimization algorithms. *arXiv preprint arXiv:1707.06347*\n","\n","### Cross-References:\n","- **Previous**: [Chapter 10: Actor-Critic Methods](chapter10_actor_critic.ipynb)\n","- **Next**: [Chapter 12: Model-Based Methods](chapter12_model_based_methods.ipynb)\n","- **Related**: [Chapter 9: Policy Gradients](chapter09_policy_gradients.ipynb)\n","\n","### Next Steps:\n","- Implement PPO on more complex environments\n","- Explore natural policy gradients and advanced trust region methods\n","- Study recent improvements like PPO2 and implementation tricks\n","\n","---\n","*This notebook is part of the Reinforcement Learning for Engineer-Mathematicians textbook. For complete bibliography, see [bibliography.md](bibliography.md)*"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}