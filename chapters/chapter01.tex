\chapter{Introduction and Mathematical Prerequisites}
\label{ch:introduction}

\begin{keyideabox}[Chapter Overview]
This chapter introduces the fundamental mathematical tools needed for reinforcement learning and provides intuitive motivation for why RL represents a paradigm shift from classical control theory. We'll cover probability theory, linear algebra, optimization, and stochastic processes with practical examples.
\end{keyideabox}

\section{Motivation: From Control Theory to Learning Systems}

\begin{intuitionbox}[Why Reinforcement Learning?]
Imagine teaching a child to ride a bicycle. You don't give them the equations of motion or tell them exactly how to balance. Instead, they learn through trial and error, gradually improving their balance and control. This is the essence of reinforcement learning.
\end{intuitionbox}

Reinforcement learning represents a fundamental paradigm shift from classical control theory and optimization. While traditional engineering approaches rely on explicit models and well-defined objectives, reinforcement learning enables systems to learn optimal behavior through interaction with their environment.

\begin{examplebox}[Traditional Control vs. Reinforcement Learning]
Consider a classic engineering problem: designing a controller for an inverted pendulum.

\textbf{Traditional Control Approach:}
\begin{enumerate}
    \item Deriving the system dynamics using Lagrangian mechanics
    \item Linearizing around the equilibrium point  
    \item Designing a feedback controller using pole placement or LQR
    \item Implementing the controller with known parameters
\end{enumerate}

\textbf{Reinforcement Learning Approach:}
\begin{enumerate}
    \item Define states (angle, angular velocity) and actions (applied force)
    \item Specify a reward function (positive for upright, negative for falling)
    \item Allow the agent to explore and learn through trial and error
    \item Converge to an optimal policy without explicit knowledge of dynamics
\end{enumerate}
\end{examplebox}

This fundamental difference opens up possibilities for systems where:
\begin{itemize}
    \item Dynamics are unknown or too complex to model accurately
    \item Environment conditions change over time
    \item Multiple conflicting objectives must be balanced
    \item System parameters vary or degrade over time
\end{itemize}

\begin{examplebox}[Industrial Example: Power Grid Management]
Modern power grids face unprecedented challenges with renewable energy integration, electric vehicle charging, and dynamic pricing. Traditional grid control relies on pre-computed lookup tables and heuristic rules. Reinforcement learning enables real-time optimization that adapts to:
\begin{itemize}
    \item Variable renewable generation
    \item Changing demand patterns
    \item Equipment failures and network topology changes
    \item Market price fluctuations
\end{itemize}
\end{examplebox}

\section{Mathematical Notation and Conventions}

\begin{notebox}[Notation Guide]
Throughout this book, we adopt consistent mathematical notation that aligns with both control theory and machine learning conventions. Don't worry if some symbols are unfamiliar nowâ€”we'll introduce them gradually with intuitive explanations.
\end{notebox}

Throughout this book, we adopt consistent mathematical notation that aligns with both control theory and machine learning conventions.

\subsection{Sets and Spaces}

\begin{intuitionbox}[Understanding Spaces]
Think of a "space" as the collection of all possible values a variable can take. For example, if we're controlling a robot arm, the state space might include all possible joint angles and velocities.
\end{intuitionbox}

\begin{align}
\state &= \text{State space (all possible states)} \\
\action &= \text{Action space (all possible actions)} \\
\reward &= \text{Reward space (all possible rewards)} \\
\real^n &= \text{$n$-dimensional real vector space} \\
\real^{m \times n} &= \text{Space of $m \times n$ real matrices}
\end{align}

\subsection{Functions and Operators}
\begin{align}
\policy: \state \to \action &\quad \text{(Deterministic policy)} \\
\policy: \state \to \Delta(\action) &\quad \text{(Stochastic policy)} \\
\valuefunction^\policy: \state \to \real &\quad \text{(Value function)} \\
\qvalue^\policy: \state \times \action \to \real &\quad \text{(Action-value function)} \\
T: \real^\state \to \real^\state &\quad \text{(Bellman operator)}
\end{align}

where $\Delta(\action)$ denotes the space of probability distributions over $\action$.

\subsection{Probability and Expectation}
\begin{align}
\prob(s'|s,a) &= \text{Transition probability} \\
\expect_\policy[\cdot] &= \text{Expectation under policy $\policy$} \\
\expect_{s \sim \mu}[\cdot] &= \text{Expectation over distribution $\mu$}
\end{align}

\section{Probability Theory Refresher}

Reinforcement learning is fundamentally about making decisions under uncertainty. A solid understanding of probability theory is essential for analyzing convergence properties, sample complexity, and algorithm performance.

\subsection{Probability Spaces and Random Variables}

\begin{definition}[Probability Space]
A probability space is a triple $(\Omega, \mathcal{F}, \prob)$ where:
\begin{itemize}
    \item $\Omega$ is the sample space (set of all possible outcomes)
    \item $\mathcal{F}$ is a $\sigma$-algebra on $\Omega$ (collection of measurable events)
    \item $\prob: \mathcal{F} \to [0,1]$ is a probability measure satisfying:
    \begin{enumerate}
        \item $\prob(\Omega) = 1$
        \item For disjoint events $A_1, A_2, \ldots$: $\prob(\bigcup_{i=1}^\infty A_i) = \sum_{i=1}^\infty \prob(A_i)$
    \end{enumerate}
\end{itemize}
\end{definition}

\begin{definition}[Random Variable]
A random variable $X$ is a measurable function $X: \Omega \to \real$ such that for every Borel set $B \subseteq \real$, the preimage $X^{-1}(B) \in \mathcal{F}$.
\end{definition}

\subsection{Conditional Expectation and Martingales}

Conditional expectation plays a crucial role in reinforcement learning, particularly in the analysis of temporal difference methods and policy gradient algorithms.

\begin{definition}[Conditional Expectation]
Given random variables $X$ and $Y$, the conditional expectation $\expect[X|Y]$ is the unique (almost surely) random variable that is:
\begin{enumerate}
    \item Measurable with respect to $\sigma(Y)$
    \item Satisfies $\expect[\expect[X|Y] \cdot \mathbf{1}_A] = \expect[X \cdot \mathbf{1}_A]$ for all $A \in \sigma(Y)$
\end{enumerate}
\end{definition}

\begin{theorem}[Law of Total Expectation]
For random variables $X$ and $Y$:
\begin{equation}
\expect[X] = \expect[\expect[X|Y]]
\end{equation}
\end{theorem}

\begin{definition}[Martingale]
A sequence of random variables $\{X_t\}_{t=0}^\infty$ is a martingale with respect to filtration $\{\mathcal{F}_t\}_{t=0}^\infty$ if:
\begin{enumerate}
    \item $X_t$ is $\mathcal{F}_t$-measurable for all $t$
    \item $\expect[|X_t|] < \infty$ for all $t$
    \item $\expect[X_{t+1}|\mathcal{F}_t] = X_t$ almost surely
\end{enumerate}
\end{definition}

Martingales are fundamental for proving convergence of stochastic algorithms in reinforcement learning.

\subsection{Concentration Inequalities}

Concentration inequalities provide bounds on the probability that random variables deviate from their expected values. These are essential for finite-sample analysis of RL algorithms.

\begin{theorem}[Hoeffding's Inequality]
Let $X_1, \ldots, X_n$ be independent random variables with $X_i \in [a_i, b_i]$ almost surely. Then for any $t > 0$:
\begin{equation}
\prob\left(\left|\frac{1}{n}\sum_{i=1}^n X_i - \frac{1}{n}\sum_{i=1}^n \expect[X_i]\right| \geq t\right) \leq 2\exp\left(-\frac{2n^2t^2}{\sum_{i=1}^n(b_i-a_i)^2}\right)
\end{equation}
\end{theorem}

\begin{theorem}[Azuma's Inequality]
Let $\{X_t\}_{t=0}^\infty$ be a martingale with respect to $\{\mathcal{F}_t\}_{t=0}^\infty$ such that $|X_{t+1} - X_t| \leq c_t$ almost surely. Then:
\begin{equation}
\prob(|X_n - X_0| \geq t) \leq 2\exp\left(-\frac{t^2}{2\sum_{i=0}^{n-1}c_i^2}\right)
\end{equation}
\end{theorem}

\section{Linear Algebra Essentials}

Linear algebra provides the foundation for function approximation, policy parameterization, and many algorithmic techniques in reinforcement learning.

\subsection{Vector Spaces and Norms}

\begin{definition}[Vector Space]
A vector space $V$ over field $\mathbb{F}$ (typically $\real$ or $\mathbb{C}$) is a set equipped with vector addition and scalar multiplication satisfying:
\begin{enumerate}
    \item Commutativity: $u + v = v + u$
    \item Associativity: $(u + v) + w = u + (v + w)$
    \item Identity: $\exists 0 \in V$ such that $v + 0 = v$
    \item Inverse: $\forall v \in V, \exists -v$ such that $v + (-v) = 0$
    \item Scalar associativity: $a(bv) = (ab)v$
    \item Scalar identity: $1v = v$
    \item Distributivity: $a(u + v) = au + av$ and $(a + b)v = av + bv$
\end{enumerate}
\end{definition}

\begin{definition}[Norm]
A norm on vector space $V$ is a function $\|\cdot\|: V \to \real_{\geq 0}$ satisfying:
\begin{enumerate}
    \item $\|v\| = 0$ if and only if $v = 0$
    \item $\|av\| = |a|\|v\|$ for scalar $a$
    \item $\|u + v\| \leq \|u\| + \|v\|$ (triangle inequality)
\end{enumerate}
\end{definition}

Common norms in $\real^n$:
\begin{align}
\|x\|_1 &= \sum_{i=1}^n |x_i| \quad \text{($\ell_1$ norm)} \\
\|x\|_2 &= \sqrt{\sum_{i=1}^n x_i^2} \quad \text{(Euclidean norm)} \\
\|x\|_\infty &= \max_{i=1,\ldots,n} |x_i| \quad \text{($\ell_\infty$ norm)}
\end{align}

\subsection{Inner Products and Orthogonality}

\begin{definition}[Inner Product]
An inner product on real vector space $V$ is a function $\langle \cdot, \cdot \rangle: V \times V \to \real$ satisfying:
\begin{enumerate}
    \item Symmetry: $\langle u, v \rangle = \langle v, u \rangle$
    \item Linearity: $\langle au + bv, w \rangle = a\langle u, w \rangle + b\langle v, w \rangle$
    \item Positive definiteness: $\langle v, v \rangle \geq 0$ with equality iff $v = 0$
\end{enumerate}
\end{definition}

The induced norm is $\|v\| = \sqrt{\langle v, v \rangle}$.

\begin{theorem}[Cauchy-Schwarz Inequality]
For vectors $u, v$ in an inner product space:
\begin{equation}
|\langle u, v \rangle| \leq \|u\| \|v\|
\end{equation}
with equality if and only if $u$ and $v$ are linearly dependent.
\end{theorem}

\subsection{Eigenvalues and Spectral Theory}

\begin{definition}[Eigenvalue and Eigenvector]
For matrix $A \in \real^{n \times n}$, scalar $\lambda$ is an eigenvalue with corresponding eigenvector $v \neq 0$ if:
\begin{equation}
Av = \lambda v
\end{equation}
\end{definition}

\begin{theorem}[Spectral Theorem for Symmetric Matrices]
Every real symmetric matrix $A$ has an orthonormal basis of eigenvectors with real eigenvalues. If $A = Q\Lambda Q^T$ where $Q$ is orthogonal and $\Lambda$ is diagonal, then:
\begin{equation}
A = \sum_{i=1}^n \lambda_i q_i q_i^T
\end{equation}
where $\lambda_i$ are eigenvalues and $q_i$ are corresponding orthonormal eigenvectors.
\end{theorem}

\section{Optimization Fundamentals}

Optimization theory underpins virtually all reinforcement learning algorithms, from value iteration to policy gradient methods.

\subsection{Convex Analysis}

\begin{definition}[Convex Set]
A set $C \subseteq \real^n$ is convex if for all $x, y \in C$ and $\lambda \in [0,1]$:
\begin{equation}
\lambda x + (1-\lambda) y \in C
\end{equation}
\end{definition}

\begin{definition}[Convex Function]
A function $f: \real^n \to \real$ is convex if its domain is convex and for all $x, y$ in the domain and $\lambda \in [0,1]$:
\begin{equation}
f(\lambda x + (1-\lambda) y) \leq \lambda f(x) + (1-\lambda) f(y)
\end{equation}
\end{definition}

\begin{theorem}[First-Order Characterization of Convexity]
For differentiable function $f$, the following are equivalent:
\begin{enumerate}
    \item $f$ is convex
    \item $f(y) \geq f(x) + \nabla f(x)^T(y - x)$ for all $x, y$
    \item $\nabla f$ is monotone: $(\nabla f(x) - \nabla f(y))^T(x - y) \geq 0$
\end{enumerate}
\end{theorem}

\subsection{Unconstrained Optimization}

\begin{theorem}[Necessary Conditions for Optimality]
If $x^*$ is a local minimum of differentiable function $f$, then:
\begin{equation}
\nabla f(x^*) = 0
\end{equation}
If $f$ is twice differentiable, then additionally:
\begin{equation}
\nabla^2 f(x^*) \succeq 0 \quad \text{(positive semidefinite)}
\end{equation}
\end{theorem}

\begin{theorem}[Sufficient Conditions for Optimality]
If $\nabla f(x^*) = 0$ and $\nabla^2 f(x^*) \succ 0$ (positive definite), then $x^*$ is a strict local minimum.
\end{theorem}

\subsection{Gradient Descent and Convergence Analysis}

The gradient descent algorithm iterates:
\begin{equation}
x_{k+1} = x_k - \alpha_k \nabla f(x_k)
\end{equation}

\begin{theorem}[Convergence of Gradient Descent]
For convex function $f$ with $L$-Lipschitz gradient and step size $\alpha \leq 1/L$:
\begin{equation}
f(x_k) - f(x^*) \leq \frac{\|x_0 - x^*\|^2}{2\alpha k}
\end{equation}
where $x^*$ is the optimal solution.
\end{theorem}

For strongly convex functions, the convergence rate improves to exponential.

\section{Stochastic Processes and Markov Chains}

Understanding stochastic processes is crucial for analyzing the temporal dynamics of reinforcement learning systems.

\subsection{Discrete-Time Stochastic Processes}

\begin{definition}[Stochastic Process]
A discrete-time stochastic process is a sequence of random variables $\{X_t\}_{t=0}^\infty$ where each $X_t$ takes values in some state space $\state$.
\end{definition}

\begin{definition}[Markov Property]
A stochastic process $\{X_t\}_{t=0}^\infty$ satisfies the Markov property if:
\begin{equation}
\prob(X_{t+1} = s' | X_t = s, X_{t-1} = s_{t-1}, \ldots, X_0 = s_0) = \prob(X_{t+1} = s' | X_t = s)
\end{equation}
for all states $s, s', s_0, \ldots, s_{t-1}$ and times $t \geq 0$.
\end{definition}

\subsection{Markov Chain Analysis}

For finite state space $\state = \{1, 2, \ldots, n\}$, a Markov chain is characterized by its transition matrix $P \in \real^{n \times n}$ where $P_{ij} = \prob(X_{t+1} = j | X_t = i)$.

\begin{definition}[Irreducibility and Aperiodicity]
A Markov chain is:
\begin{itemize}
    \item \textbf{Irreducible} if every state is reachable from every other state
    \item \textbf{Aperiodic} if $\gcd\{n \geq 1 : P_{ii}^{(n)} > 0\} = 1$ for some state $i$
\end{itemize}
\end{definition}

\begin{theorem}[Fundamental Theorem of Markov Chains]
For an irreducible, aperiodic, finite Markov chain:
\begin{enumerate}
    \item There exists a unique stationary distribution $\pi$ satisfying $\pi = \pi P$
    \item $\lim_{t \to \infty} P^t = \mathbf{1}\pi^T$ where $\mathbf{1}$ is the vector of ones
    \item For any initial distribution $\mu_0$: $\lim_{t \to \infty} \|\mu_t - \pi\|_{TV} = 0$
\end{enumerate}
\end{theorem}

\subsection{Mixing Times and Convergence Rates}

\begin{definition}[Total Variation Distance]
The total variation distance between distributions $\mu$ and $\nu$ on finite space $\state$ is:
\begin{equation}
\|\mu - \nu\|_{TV} = \frac{1}{2}\sum_{s \in \state} |\mu(s) - \nu(s)|
\end{equation}
\end{definition}

\begin{definition}[Mixing Time]
The mixing time of a Markov chain is:
\begin{equation}
t_{mix}(\epsilon) = \min\{t : \max_{i \in \state} \|P^t(i, \cdot) - \pi\|_{TV} \leq \epsilon\}
\end{equation}
\end{definition}

Understanding mixing times is essential for analyzing sample complexity in reinforcement learning algorithms that rely on sampling from stationary distributions.

\section{Chapter Summary}

This chapter established the mathematical foundations necessary for rigorous analysis of reinforcement learning algorithms. Key concepts include:

\begin{itemize}
    \item The paradigm shift from model-based control to learning-based optimization
    \item Probability theory tools: conditional expectation, martingales, concentration inequalities
    \item Linear algebra foundations: vector spaces, norms, spectral theory
    \item Convex optimization and gradient descent convergence analysis
    \item Markov chain theory and convergence to stationary distributions
\end{itemize}

These mathematical tools will be applied throughout the book to analyze algorithm convergence, sample complexity, and performance guarantees. The next chapter develops the formal framework of Markov Decision Processes, which provides the mathematical foundation for all subsequent reinforcement learning algorithms.