\chapter{Actor-Critic Methods}
\label{ch:actor-critic}

\begin{keyideabox}[Chapter Overview]
Actor-critic methods combine the best aspects of policy gradient and value-based approaches by maintaining both a policy (actor) and a value function (critic). The critic reduces variance in policy gradient estimates while the actor handles continuous action spaces and stochastic policies. This chapter covers the theoretical foundations, practical algorithms, and advanced techniques including asynchronous methods and natural actor-critic approaches.
\end{keyideabox}

\begin{intuitionbox}[Two-Brain Learning]
Think of learning to drive a car with an instructor. The student (actor) controls the steering, acceleration, and braking based on their current policy. The instructor (critic) observes the outcomes and provides feedback: "that lane change was worth +5 points" or "braking too late cost -10 points." The student adjusts their driving policy based on this evaluation, while the instructor improves their ability to assess driving quality. Both learn simultaneously and help each other improve.
\end{intuitionbox}

\section{Motivation and Architecture}

\subsection{Limitations of Pure Approaches}

Pure policy gradient methods suffer from high variance, while pure value-based methods are limited to discrete actions. Actor-critic methods address both issues:

\begin{itemize}
    \item \textbf{Variance Reduction}: Critics provide low-variance value estimates
    \item \textbf{Continuous Actions}: Actors can parameterize continuous policies  
    \item \textbf{Online Learning}: Updates can be made after each step
    \item \textbf{Stability}: Value function learning stabilizes policy updates
\end{itemize}

\subsection{Actor-Critic Architecture}

The actor-critic framework consists of two components:

\textbf{Actor} $\pi(a|s; \theta)$: The policy that selects actions
\textbf{Critic} $V(s; w)$ or $Q(s,a; w)$: The value function that evaluates actions

\begin{figure}[h]
\centering
\begin{tikzpicture}[node distance=2cm]
    \node[rectangle, draw, minimum width=2cm] (env) {Environment};
    \node[rectangle, draw, above left=of env] (actor) {Actor $\pi(a|s;\theta)$};
    \node[rectangle, draw, above right=of env] (critic) {Critic $V(s;w)$};
    
    \draw[->] (env) to node[left] {$s_t$} (actor);
    \draw[->] (actor) to node[above left] {$a_t$} (env);
    \draw[->] (env) to node[right] {$s_t, r_t$} (critic);
    \draw[->] (critic) to node[above] {$V(s_t)$} (actor);
    \draw[->] (env) to node[below] {$r_t, s_{t+1}$} (env);
\end{tikzpicture}
\caption{Actor-critic architecture showing information flow}
\end{figure}

\section{Basic Actor-Critic Algorithm}

\subsection{Value Function Critic}

Using a state value function $V(s; w)$ as the critic, the actor update becomes:
\begin{equation}
\theta_{t+1} = \theta_t + \alpha_\theta \gamma^t \delta_t \nabla_\theta \log \pi(a_t|s_t; \theta_t)
\end{equation}

where the TD error is:
\begin{equation}
\delta_t = r_t + \gamma V(s_{t+1}; w_t) - V(s_t; w_t)
\end{equation}

The critic is updated using standard TD learning:
\begin{equation}
w_{t+1} = w_t + \alpha_w \delta_t \nabla_w V(s_t; w_t)
\end{equation}

\begin{algorithm}
\caption{Basic Actor-Critic}
\begin{algorithmic}
\REQUIRE Learning rates $\alpha_\theta, \alpha_w$
\STATE Initialize actor parameters $\theta$ and critic parameters $w$
\FOR{episode = 1, $M$}
    \STATE Initialize state $s_0$
    \FOR{$t = 0, T-1$}
        \STATE Sample action $a_t \sim \pi(\cdot|s_t; \theta)$
        \STATE Execute $a_t$, observe $r_t, s_{t+1}$
        \STATE $\delta_t \leftarrow r_t + \gamma V(s_{t+1}; w) - V(s_t; w)$ \COMMENT{TD error}
        \STATE $w \leftarrow w + \alpha_w \delta_t \nabla_w V(s_t; w)$ \COMMENT{Critic update}
        \STATE $\theta \leftarrow \theta + \alpha_\theta \gamma^t \delta_t \nabla_\theta \log \pi(a_t|s_t; \theta)$ \COMMENT{Actor update}
    \ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsection{Convergence Properties}

\begin{theorem}[Actor-Critic Convergence]
Under appropriate conditions (bounded rewards, function approximation errors, learning rates), the basic actor-critic algorithm converges to a local optimum of the policy performance measure.
\end{theorem}

Key conditions include:
\begin{itemize}
    \item Compatible function approximation for the critic
    \item Appropriate learning rate schedules
    \item Exploration ensuring all state-action pairs are visited
    \item Bounded approximation errors
\end{itemize}

\section{Advanced Actor-Critic Variants}

\subsection{Advantage Actor-Critic (A2C)}

Instead of using TD error directly, A2C uses the advantage function:
\begin{equation}
A(s,a) = Q(s,a) - V(s)
\end{equation}

For the one-step case:
\begin{equation}
A(s_t, a_t) \approx r_t + \gamma V(s_{t+1}) - V(s_t) = \delta_t
\end{equation}

\subsection{Generalized Advantage Estimation (GAE)}

GAE provides a family of advantage estimators trading off bias and variance:
\begin{equation}
\hat{A}_t^{(\lambda)} = \sum_{l=0}^\infty (\gamma \lambda)^l \delta_{t+l}
\end{equation}

where $\delta_{t+l} = r_{t+l} + \gamma V(s_{t+l+1}) - V(s_{t+l})$.

\textbf{Special Cases:}
\begin{itemize}
    \item $\lambda = 0$: $\hat{A}_t^{(0)} = \delta_t$ (high bias, low variance)
    \item $\lambda = 1$: $\hat{A}_t^{(1)} = \sum_{l=0}^\infty \gamma^l \delta_{t+l}$ (low bias, high variance)
\end{itemize}

\begin{remarkbox}[GAE Practical Implementation]
In practice, GAE is computed backward through the trajectory:
\begin{align}
\hat{A}_t^{(\lambda)} &= \delta_t + \gamma \lambda \hat{A}_{t+1}^{(\lambda)} \\
\delta_t &= r_t + \gamma V(s_{t+1}) - V(s_t)
\end{align}
This recursive formulation is both computationally efficient and numerically stable.
\end{remarkbox}

\section{Asynchronous Advantage Actor-Critic (A3C)}

A3C parallelizes learning using multiple workers that interact with separate environment instances.

\subsection{Asynchronous Architecture}

\begin{itemize}
    \item \textbf{Global Network}: Shared parameters $\theta_{global}, w_{global}$
    \item \textbf{Worker Networks}: Local copies that accumulate gradients
    \item \textbf{Asynchronous Updates}: Workers update global parameters independently
\end{itemize}

\begin{algorithm}
\caption{A3C Worker Process}
\begin{algorithmic}
\REQUIRE Global parameters $\theta_{global}, w_{global}$, local parameters $\theta, w$
\REPEAT
    \STATE $\theta \leftarrow \theta_{global}, w \leftarrow w_{global}$ \COMMENT{Sync with global}
    \STATE $t_{start} \leftarrow t$
    \STATE $s \leftarrow$ current state
    \REPEAT
        \STATE Perform action $a_t \sim \pi(a_t|s_t; \theta)$
        \STATE Receive reward $r_t$ and new state $s_{t+1}$
        \STATE $t \leftarrow t + 1$
    \UNTIL{terminal or $t - t_{start} = t_{max}$}
    \STATE $R \leftarrow \begin{cases} 0 & \text{if } s_t \text{ is terminal} \\ V(s_t; w) & \text{otherwise} \end{cases}$
    \FOR{$i = t-1, \ldots, t_{start}$}
        \STATE $R \leftarrow r_i + \gamma R$
        \STATE Accumulate gradients: $d\theta \leftarrow d\theta + \nabla_\theta \log \pi(a_i|s_i; \theta)(R - V(s_i; w))$
        \STATE Accumulate gradients: $dw \leftarrow dw + \nabla_w (R - V(s_i; w))^2$
    \ENDFOR
    \STATE Update global parameters: $\theta_{global} \leftarrow \theta_{global} + \alpha_\theta d\theta$
    \STATE Update global parameters: $w_{global} \leftarrow w_{global} + \alpha_w dw$
\UNTIL{global termination condition}
\end{algorithmic}
\end{algorithm}

\subsection{Benefits of Asynchronous Learning}

\begin{enumerate}
    \item \textbf{Data Efficiency}: Multiple workers explore simultaneously
    \item \textbf{Stability}: Decorrelated experiences improve stability
    \item \textbf{Exploration}: Different workers can pursue different exploration strategies
    \item \textbf{Computational Efficiency}: CPU-based parallelization
\end{enumerate}

\section{Natural Actor-Critic}

Natural actor-critic methods use the natural policy gradient in the actor update.

\subsection{Compatible Function Approximation}

For unbiased natural policy gradients, the critic must satisfy the compatibility condition:

\begin{definition}[Compatible Function Approximation]
A function approximator $f(s,a; w)$ is compatible with policy $\pi(a|s; \theta)$ if:
\begin{enumerate}
    \item $\nabla_w f(s,a; w) = \nabla_\theta \log \pi(a|s; \theta)$
    \item $w$ minimizes the mean-squared error: $\min_w \mathbb{E} [(f(s,a; w) - Q^\pi(s,a))^2]$
\end{enumerate}
\end{definition}

\begin{theorem}[Natural Actor-Critic Policy Gradient]
If the critic uses compatible function approximation, then:
\begin{equation}
\mathbb{E} \left[ \nabla_\theta \log \pi(a|s; \theta) f(s,a; w) \right] = \nabla_\theta J(\theta)
\end{equation}
is the natural policy gradient.
\end{theorem}

\subsection{Natural Actor-Critic Algorithm}

\begin{algorithm}
\caption{Natural Actor-Critic}
\begin{algorithmic}
\REQUIRE Learning rates $\alpha_\theta, \alpha_w$
\STATE Initialize $\theta, w$
\FOR{episode = 1, $M$}
    \FOR{$t = 0, T-1$}
        \STATE Sample $a_t \sim \pi(\cdot|s_t; \theta)$, observe $r_t, s_{t+1}$
        \STATE $\delta_t \leftarrow r_t + \gamma V(s_{t+1}) - V(s_t)$
        \STATE $w \leftarrow w + \alpha_w \delta_t \nabla_\theta \log \pi(a_t|s_t; \theta)$ \COMMENT{Compatible critic}
        \STATE $\theta \leftarrow \theta + \alpha_\theta \nabla_\theta \log \pi(a_t|s_t; \theta) f(s_t, a_t; w)$ \COMMENT{Natural gradient}
    \ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}

\section{Bias-Variance Analysis}

\subsection{Sources of Bias and Variance}

\textbf{Actor Bias Sources:}
\begin{itemize}
    \item Value function approximation errors
    \item Bootstrap bias from TD learning
    \item Function approximation in policy parameterization
\end{itemize}

\textbf{Actor Variance Sources:}
\begin{itemize}
    \item Policy gradient estimation
    \item Environment stochasticity
    \item Value function learning noise
\end{itemize}

\textbf{Critic Bias Sources:}
\begin{itemize}
    \item Bootstrap approximation
    \item Function approximation limitations
    \item Off-policy sampling (if applicable)
\end{itemize}

\textbf{Critic Variance Sources:}
\begin{itemize}
    \item Reward noise
    \item Policy changes during learning
    \item Sample-based updates
\end{itemize}

\subsection{Bias-Variance Tradeoffs}

\begin{examplebox}[Multi-Step Returns]
Consider $n$-step returns for advantage estimation:
\begin{equation}
A_t^{(n)} = \sum_{k=0}^{n-1} \gamma^k r_{t+k} + \gamma^n V(s_{t+n}) - V(s_t)
\end{equation}

\textbf{Bias-Variance Analysis:}
\begin{itemize}
    \item $n = 1$: High bias (bootstrap approximation), low variance
    \item $n = \infty$: Low bias (Monte Carlo), high variance  
    \item Intermediate $n$: Balanced bias-variance tradeoff
\end{itemize}

GAE with parameter $\lambda$ provides a weighted combination of all $n$-step returns.
\end{examplebox}

\section{Off-Policy Actor-Critic}

\subsection{Importance Sampling Correction}

For off-policy learning, correct the policy gradient using importance sampling:
\begin{equation}
\nabla_\theta J(\theta) = \mathbb{E}_{\pi_{\text{old}}} \left[ \frac{\pi(a|s; \theta)}{\pi_{\text{old}}(a|s)} \nabla_\theta \log \pi(a|s; \theta) A(s,a) \right]
\end{equation}

\subsection{Off-Policy Actor-Critic (Off-PAC)}

\begin{algorithm}
\caption{Off-Policy Actor-Critic}
\begin{algorithmic}
\REQUIRE Behavior policy $\pi_{\text{old}}$, replay buffer $\mathcal{D}$
\FOR{iteration = 1, $K$}
    \STATE Sample batch $\{(s_i, a_i, r_i, s_i')\}$ from $\mathcal{D}$
    \FOR{each transition $(s_i, a_i, r_i, s_i')$}
        \STATE $\rho_i \leftarrow \frac{\pi(a_i|s_i; \theta)}{\pi_{\text{old}}(a_i|s_i)}$ \COMMENT{Importance ratio}
        \STATE $\delta_i \leftarrow r_i + \gamma V(s_i') - V(s_i)$ \COMMENT{TD error}
        \STATE Accumulate critic gradient: $\nabla_w V(s_i) \delta_i$
        \STATE Accumulate actor gradient: $\rho_i \nabla_\theta \log \pi(a_i|s_i; \theta) \delta_i$
    \ENDFOR
    \STATE Update parameters using accumulated gradients
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsection{Variance Issues with Importance Sampling}

Importance sampling can have extremely high variance when $\frac{\pi(a|s)}{\pi_{\text{old}}(a|s)}$ is large. Mitigation strategies include:

\begin{itemize}
    \item \textbf{Clipping}: $\rho = \min(c, \frac{\pi(a|s)}{\pi_{\text{old}}(a|s)})$
    \item \textbf{Truncated Importance Sampling}: Only use samples with $\rho \leq \text{threshold}$
    \item \textbf{Control Variates}: Use additional baselines to reduce variance
\end{itemize}

\section{Deterministic Policy Gradients}

For deterministic policies $a = \mu(s; \theta)$, the policy gradient becomes:

\begin{theorem}[Deterministic Policy Gradient]
For deterministic policy $\mu(s; \theta)$:
\begin{equation}
\nabla_\theta J(\theta) = \mathbb{E}_{\rho^\mu} \left[ \nabla_\theta \mu(s; \theta) \nabla_a Q(s,a; w) \big|_{a=\mu(s; \theta)} \right]
\end{equation}
where $\rho^\mu$ is the state distribution under policy $\mu$.
\end{theorem}

\subsection{Deterministic Actor-Critic Algorithm}

\begin{algorithm}
\caption{Deterministic Actor-Critic}
\begin{algorithmic}
\REQUIRE Exploration noise process $\mathcal{N}$
\FOR{episode = 1, $M$}
    \FOR{$t = 0, T-1$}
        \STATE $a_t \leftarrow \mu(s_t; \theta) + \mathcal{N}_t$ \COMMENT{Add exploration noise}
        \STATE Execute $a_t$, observe $r_t, s_{t+1}$
        \STATE Update critic: $w \leftarrow w + \alpha_w \delta_t \nabla_w Q(s_t, a_t; w)$
        \STATE Update actor: $\theta \leftarrow \theta + \alpha_\theta \nabla_\theta \mu(s_t; \theta) \nabla_a Q(s_t, a; w)|_{a=\mu(s_t; \theta)}$
    \ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}

\section{Implementation Considerations}

\subsection{Network Architecture Design}

\textbf{Shared vs Separate Networks:}
\begin{itemize}
    \item \textbf{Shared}: Common hidden layers with separate heads for actor and critic
    \item \textbf{Separate}: Independent networks with different architectures
\end{itemize}

\textbf{Shared Network Advantages:}
\begin{itemize}
    \item Parameter efficiency
    \item Shared feature learning
    \item Faster training with good features
\end{itemize}

\textbf{Separate Network Advantages:}
\begin{itemize}
    \item Independent learning rates
    \item Specialized architectures
    \item Reduced interference between actor and critic
\end{itemize}

\subsection{Learning Rate Considerations}

Typical learning rate relationships:
\begin{itemize}
    \item Critic learning rate $\alpha_w$: Higher (faster value learning)
    \item Actor learning rate $\alpha_\theta$: Lower (stable policy updates)
    \item Common ratio: $\alpha_w = 10 \alpha_\theta$
\end{itemize}

\subsection{Exploration in Actor-Critic}

\textbf{Stochastic Policies}: Natural exploration through policy randomness
\textbf{Deterministic Policies}: Require explicit exploration mechanisms:
\begin{itemize}
    \item Gaussian noise: $a = \mu(s) + \epsilon, \epsilon \sim \mathcal{N}(0, \sigma^2)$
    \item Ornstein-Uhlenbeck process: Temporally correlated noise
    \item Parameter space noise: Add noise to network parameters
\end{itemize}

\section{Advanced Topics}

\subsection{Multi-Agent Actor-Critic}

For multi-agent systems, each agent maintains its own actor-critic:
\begin{itemize}
    \item \textbf{Independent Learning}: Agents ignore each other's existence
    \item \textbf{Centralized Training, Decentralized Execution}: Share information during training
    \item \textbf{Communication}: Agents exchange information during execution
\end{itemize}

\subsection{Hierarchical Actor-Critic}

For hierarchical policies with multiple levels:
\begin{itemize}
    \item \textbf{High-level Actor}: Selects goals or sub-policies
    \item \textbf{Low-level Actor}: Executes primitive actions
    \item \textbf{Hierarchical Critic}: Evaluates both levels
\end{itemize}

\subsection{Continuous-Time Actor-Critic}

For continuous-time systems, the updates become:
\begin{align}
d\theta_t &= \alpha_\theta \nabla_\theta \log \pi(a_t|s_t; \theta_t) dM_t \\
dw_t &= \alpha_w \delta_t \nabla_w V(s_t; w_t) dt
\end{align}
where $dM_t$ is a martingale increment.

\section{Experimental Analysis}

\subsection{Ablation Studies}

Key components to analyze:
\begin{itemize}
    \item \textbf{Advantage Estimation}: Compare TD($\lambda$), GAE, n-step returns
    \item \textbf{Network Architecture}: Shared vs separate networks
    \item \textbf{Learning Rates}: Actor-critic learning rate ratios
    \item \textbf{Batch Size}: Effect on variance and computational efficiency
\end{itemize}

\subsection{Performance Metrics}

\textbf{Learning Metrics:}
\begin{itemize}
    \item Sample efficiency (episodes to convergence)
    \item Final performance level
    \item Learning curve stability
    \item Wall-clock time to solution
\end{itemize}

\textbf{Diagnostic Metrics:}
\begin{itemize}
    \item Policy gradient variance
    \item Value function approximation error
    \item Actor-critic correlation
    \item Exploration effectiveness
\end{itemize}

\section{Applications}

\subsection{Robotics Control}

\begin{examplebox}[Humanoid Walking]
Training a humanoid robot to walk using actor-critic:
\begin{itemize}
    \item \textbf{State}: Joint positions, velocities, orientation, contact forces
    \item \textbf{Action}: Joint torques (continuous, high-dimensional)
    \item \textbf{Reward}: Forward velocity + stability penalties
    \item \textbf{Actor}: Neural network outputting torque commands
    \item \textbf{Critic}: Value network estimating expected future reward
\end{itemize}

The continuous nature of the control problem makes actor-critic methods particularly suitable. The critic helps reduce the variance inherent in policy gradient estimation for this high-dimensional continuous control task.
\end{examplebox}

\subsection{Financial Trading}

\begin{examplebox}[Portfolio Management]
Automated trading using actor-critic methods:
\begin{itemize}
    \item \textbf{State}: Market prices, volumes, technical indicators, news sentiment
    \item \textbf{Action}: Portfolio weights (continuous, constrained to sum to 1)
    \item \textbf{Reward}: Risk-adjusted returns (Sharpe ratio)
    \item \textbf{Actor}: Policy network outputting portfolio allocation
    \item \textbf{Critic}: Value network estimating expected portfolio performance
\end{itemize}

The stochastic nature of financial markets and the need for continuous portfolio allocation make actor-critic methods well-suited for this domain.
\end{examplebox}

\section{Chapter Summary}

Actor-critic methods provide a powerful framework that combines the advantages of both policy gradient and value-based approaches:

\begin{itemize}
    \item \textbf{Variance Reduction}: Critics provide low-variance advantage estimates
    \item \textbf{Online Learning}: Updates possible after each step
    \item \textbf{Continuous Actions}: Natural handling through policy parameterization
    \item \textbf{Stability}: Value function learning stabilizes policy updates
\end{itemize}

Key algorithmic developments include:
\begin{itemize}
    \item \textbf{Basic Actor-Critic}: Foundation algorithm with state value critic
    \item \textbf{A2C/A3C}: Advantage-based updates with asynchronous learning
    \item \textbf{GAE}: Bias-variance tradeoff in advantage estimation
    \item \textbf{Natural Actor-Critic}: Incorporating natural policy gradients
    \item \textbf{Off-Policy Methods}: Enabling sample reuse through importance sampling
\end{itemize}

Challenges and considerations:
\begin{itemize}
    \item \textbf{Bias-Variance Tradeoff}: Balancing critic bias against variance reduction
    \item \textbf{Learning Rate Tuning}: Coordinating actor and critic learning
    \item \textbf{Network Architecture}: Designing effective shared or separate networks
    \item \textbf{Exploration}: Ensuring adequate exploration in continuous spaces
\end{itemize}

\begin{keyideabox}[Key Takeaways]
\begin{enumerate}
    \item Actor-critic methods combine policy optimization with value function learning
    \item Critics reduce variance in policy gradient estimates at the cost of introducing bias
    \item Asynchronous methods enable stable parallelization and improved data efficiency
    \item Compatible function approximation ensures unbiased natural policy gradients
    \item Proper balance of actor and critic learning rates is crucial for stable training
\end{enumerate}
\end{keyideabox}

The next chapter will explore advanced policy optimization methods that build on actor-critic foundations to achieve even better performance and stability.