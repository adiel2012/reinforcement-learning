\part{Core Algorithms and Theory}

This part develops the fundamental learning algorithms that form the core of reinforcement learning. Moving beyond the dynamic programming methods of Part I, which assume complete knowledge of the MDP, we now consider algorithms that learn from experience through interaction with the environment.

We begin with Monte Carlo methods that estimate value functions from complete episodes. We then develop temporal difference learning, which enables learning from individual transitions. Finally, we examine Q-learning and SARSA, which learn action-value functions and form the foundation for more advanced algorithms.

Throughout this part, we emphasize mathematical rigor in convergence analysis while maintaining practical relevance for engineering applications. Each algorithm is developed with careful attention to assumptions, convergence conditions, and sample complexity bounds.

\input{chapters/chapter04}
\input{chapters/chapter05}
\input{chapters/chapter06}