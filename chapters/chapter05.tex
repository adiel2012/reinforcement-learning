\chapter{Temporal Difference Learning}
\label{ch:temporal-difference}

Temporal Difference (TD) learning combines ideas from Monte Carlo methods and dynamic programming, enabling learning from incomplete episodes while maintaining the model-free nature of Monte Carlo methods. This chapter develops the mathematical theory of TD learning with emphasis on convergence analysis and the fundamental bias-variance tradeoff.

\section{TD(0) Algorithm and Mathematical Analysis}

\subsection{Basic TD(0) Update}

The core insight of temporal difference learning is to use the current estimate of the successor state's value to update the current state's value:

\begin{equation}
V(S_t) \leftarrow V(S_t) + \alpha [R_{t+1} + \gamma V(S_{t+1}) - V(S_t)]
\end{equation}

The TD error is defined as:
\begin{equation}
\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t)
\end{equation}

\begin{algorithm}
\caption{Tabular TD(0) Policy Evaluation}
\begin{algorithmic}
\REQUIRE Policy $\pi$ to evaluate, step size $\alpha \in (0,1]$
\STATE Initialize $V(s) \in \real$ arbitrarily for all $s \in \mathcal{S}$, except $V(\text{terminal}) = 0$
\REPEAT
    \STATE Initialize $S$
    \REPEAT
        \STATE $A \leftarrow$ action given by $\pi$ for $S$
        \STATE Take action $A$, observe $R, S'$
        \STATE $V(S) \leftarrow V(S) + \alpha[R + \gamma V(S') - V(S)]$
        \STATE $S \leftarrow S'$
    \UNTIL{$S$ is terminal}
\UNTIL{convergence or sufficient accuracy}
\end{algorithmic}
\end{algorithm}

\subsection{Relationship to Bellman Equation}

The TD(0) update can be viewed as a stochastic approximation to the Bellman equation. The expected TD update is:

\begin{align}
\expect[\delta_t | S_t = s] &= \expect[R_{t+1} + \gamma V(S_{t+1}) - V(S_t) | S_t = s] \\
&= \sum_{s',r} p(s',r|s,\pi(s))[r + \gamma V(s') - V(s)] \\
&= (T^\pi V)(s) - V(s)
\end{align}

where $T^\pi$ is the Bellman operator for policy $\pi$.

\subsection{Convergence Analysis}

\begin{theorem}[Convergence of TD(0) - Tabular Case]
For the tabular case with appropriate step size sequence $\{\alpha_t\}$ satisfying:
\begin{align}
\sum_{t=0}^\infty \alpha_t &= \infty \\
\sum_{t=0}^\infty \alpha_t^2 &< \infty
\end{align}
TD(0) converges to $V^\pi$ with probability 1.
\end{theorem}

\begin{proof}[Proof Sketch]
The proof uses stochastic approximation theory. Define the ODE:
\begin{equation}
\frac{dV}{dt} = \expect[\delta_t | V] = T^\pi V - V
\end{equation}
Since $T^\pi$ is a contraction, the unique fixed point is $V^\pi$. The stochastic approximation theorem ensures convergence of the discrete updates to the ODE solution.
\end{proof}

\section{Bias-Variance Tradeoff in TD Methods}

\subsection{Bias Analysis}

TD(0) uses the biased estimate $R_{t+1} + \gamma V(S_{t+1})$ as a target for $V(S_t)$, while Monte Carlo uses the unbiased estimate $G_t$.

\begin{theorem}[Bias of TD Target]
The TD target $R_{t+1} + \gamma V(S_{t+1})$ has bias:
\begin{equation}
\text{Bias}[R_{t+1} + \gamma V(S_{t+1})] = \gamma[\hat{V}(S_{t+1}) - V^\pi(S_{t+1})]
\end{equation}
where $\hat{V}$ is the current estimate.
\end{theorem}

\subsection{Variance Analysis}

\begin{theorem}[Variance Comparison]
Under the assumption that value function errors are small, the variance of the TD target is approximately:
\begin{equation}
\text{Var}[R_{t+1} + \gamma V(S_{t+1})] \approx \text{Var}[R_{t+1}]
\end{equation}
while the Monte Carlo target has variance:
\begin{equation}
\text{Var}[G_t] = \text{Var}\left[\sum_{k=0}^\infty \gamma^k R_{t+k+1}\right]
\end{equation}
which is typically much larger.
\end{theorem}

\subsection{Mean Squared Error Decomposition}

\begin{equation}
\text{MSE} = \text{Bias}^2 + \text{Variance} + \text{Noise}
\end{equation}

TD methods trade increased bias for reduced variance, often resulting in lower overall MSE and faster convergence.

\section{TD(λ) and Eligibility Traces}

TD(λ) provides a family of algorithms that interpolate between TD(0) and Monte Carlo methods through the use of eligibility traces.

\subsection{Forward View: n-step Returns}

The n-step return combines rewards from the next n steps with the estimated value of the state reached after n steps:

\begin{equation}
G_t^{(n)} = R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1} R_{t+n} + \gamma^n V(S_{t+n})
\end{equation}

The n-step TD update is:
\begin{equation}
V(S_t) \leftarrow V(S_t) + \alpha[G_t^{(n)} - V(S_t)]
\end{equation}

\subsection{λ-Return}

The λ-return combines all n-step returns:
\begin{equation}
G_t^\lambda = (1-\lambda) \sum_{n=1}^\infty \lambda^{n-1} G_t^{(n)}
\end{equation}

\begin{theorem}[λ-Return Properties]
The λ-return satisfies:
\begin{align}
G_t^\lambda &= R_{t+1} + \gamma[(1-\lambda)V(S_{t+1}) + \lambda G_{t+1}^\lambda] \\
\lim_{\lambda \to 0} G_t^\lambda &= R_{t+1} + \gamma V(S_{t+1}) \quad \text{(TD(0))} \\
\lim_{\lambda \to 1} G_t^\lambda &= G_t \quad \text{(Monte Carlo)}
\end{align}
\end{theorem}

\subsection{Backward View: Eligibility Traces}

Eligibility traces provide an online, incremental implementation of TD(λ):

\begin{align}
\delta_t &= R_{t+1} + \gamma V(S_{t+1}) - V(S_t) \\
e_t(s) &= \begin{cases}
\gamma \lambda e_{t-1}(s) + 1 & \text{if } s = S_t \\
\gamma \lambda e_{t-1}(s) & \text{if } s \neq S_t
\end{cases} \\
V(s) &\leftarrow V(s) + \alpha \delta_t e_t(s) \quad \forall s
\end{align}

\begin{algorithm}
\caption{TD(λ) with Eligibility Traces}
\begin{algorithmic}
\REQUIRE Policy $\pi$, step size $\alpha$, trace decay $\lambda$
\STATE Initialize $V(s) \in \real$ arbitrarily for all $s$
\REPEAT
    \STATE Initialize $S$, $e(s) = 0$ for all $s$
    \REPEAT
        \STATE $A \leftarrow$ action given by $\pi$ for $S$
        \STATE Take action $A$, observe $R, S'$
        \STATE $\delta \leftarrow R + \gamma V(S') - V(S)$
        \STATE $e(S) \leftarrow e(S) + 1$
        \FOR{all $s$}
            \STATE $V(s) \leftarrow V(s) + \alpha \delta e(s)$
            \STATE $e(s) \leftarrow \gamma \lambda e(s)$
        \ENDFOR
        \STATE $S \leftarrow S'$
    \UNTIL{$S$ is terminal}
\UNTIL{convergence}
\end{algorithmic}
\end{algorithm}

\subsection{Equivalence Theorem}

\begin{theorem}[Forward-Backward Equivalence]
Under certain conditions, the forward view (using λ-returns) and backward view (using eligibility traces) produce identical updates when applied offline to a complete episode.
\end{theorem}

\section{Convergence Theory for Linear Function Approximation}

When the state space is large, we use function approximation:
\begin{equation}
V(s) \approx \hat{V}(s, \mathbf{w}) = \mathbf{w}^T \boldsymbol{\phi}(s)
\end{equation}

The TD(0) update becomes:
\begin{equation}
\mathbf{w}_{t+1} = \mathbf{w}_t + \alpha[R_{t+1} + \gamma \mathbf{w}_t^T \boldsymbol{\phi}(S_{t+1}) - \mathbf{w}_t^T \boldsymbol{\phi}(S_t)]\boldsymbol{\phi}(S_t)
\end{equation}

\subsection{Projected Bellman Equation}

Under linear function approximation, TD(0) converges to the solution of the projected Bellman equation:
\begin{equation}
\mathbf{w}^* = \arg\min_\mathbf{w} \|\boldsymbol{\Phi}\mathbf{w} - T^\pi(\boldsymbol{\Phi}\mathbf{w})\|_{\mathbf{D}}^2
\end{equation}

where $\boldsymbol{\Phi}$ is the feature matrix and $\mathbf{D}$ is a diagonal matrix of state visitation probabilities.

\begin{theorem}[Convergence of Linear TD(0)]
Under linear function approximation, TD(0) converges to:
\begin{equation}
\mathbf{w}^* = (\boldsymbol{\Phi}^T \mathbf{D} \boldsymbol{\Phi})^{-1} \boldsymbol{\Phi}^T \mathbf{D} \mathbf{r}^\pi
\end{equation}
where $\mathbf{r}^\pi$ is the expected reward vector.
\end{theorem}

\subsection{Error Bounds}

\begin{theorem}[Approximation Error Bound]
Let $V^*$ be the optimal value function and $\hat{V}^*$ be the best linear approximation. Then:
\begin{equation}
\|V^\pi - \hat{V}^\pi\|_{\mathbf{D}} \leq \frac{1}{1-\gamma} \min_\mathbf{w} \|V^\pi - \boldsymbol{\Phi}\mathbf{w}\|_{\mathbf{D}}
\end{equation}
\end{theorem}

\section{Comparison with Monte Carlo and DP Methods}

\subsection{Computational Complexity}

\begin{center}
\begin{tabular}{lccc}
\toprule
Method & Memory & Computation per Step & Episode Completion \\
\midrule
DP & $O(|\mathcal{S}|^2|\mathcal{A}|)$ & $O(|\mathcal{S}|^2|\mathcal{A}|)$ & Not Required \\
MC & $O(|\mathcal{S}|)$ & $O(1)$ & Required \\
TD & $O(|\mathcal{S}|)$ & $O(1)$ & Not Required \\
\bottomrule
\end{tabular}
\end{center}

\subsection{Sample Efficiency}

\begin{theorem}[Sample Complexity Comparison]
Under certain regularity conditions:
\begin{itemize}
    \item TD methods: $O(\frac{1}{\epsilon^2(1-\gamma)^2})$ samples for $\epsilon$-accuracy
    \item MC methods: $O(\frac{1}{\epsilon^2(1-\gamma)^4})$ samples for $\epsilon$-accuracy
\end{itemize}
\end{theorem}

TD methods often have better sample efficiency due to lower variance, despite being biased.

\subsection{Bootstrapping vs. Sampling}

\textbf{Bootstrapping:} Using estimates of successor states (DP, TD)
\textbf{Sampling:} Using actual experience (MC, TD)

TD methods combine both, leading to:
\begin{itemize}
    \item Faster learning than MC (bootstrapping)
    \item Model-free nature (sampling)
    \item Online learning capability
\end{itemize}

\section{Advanced Topics}

\subsection{Multi-step Methods}

The n-step TD methods generalize between TD(0) and Monte Carlo:
\begin{equation}
V(S_t) \leftarrow V(S_t) + \alpha[G_t^{(n)} - V(S_t)]
\end{equation}

\begin{theorem}[Optimal Step Size]
For n-step methods, there exists an optimal n that minimizes mean squared error, typically $n \in [3, 10]$ for many problems.
\end{theorem}

\subsection{True Online TD(λ)}

The classical TD(λ) is not equivalent to the forward view when using function approximation. True online TD(λ) corrects this:

\begin{align}
\mathbf{w}_{t+1} &= \mathbf{w}_t + \alpha \delta_t \mathbf{z}_t + \alpha(\mathbf{w}_t^T \boldsymbol{\phi}_t - \mathbf{w}_{t-1}^T \boldsymbol{\phi}_t)(\mathbf{z}_t - \boldsymbol{\phi}_t) \\
\mathbf{z}_{t+1} &= \gamma \lambda \mathbf{z}_t + \boldsymbol{\phi}_{t+1} - \alpha \gamma \lambda (\mathbf{z}_t^T \boldsymbol{\phi}_{t+1})\boldsymbol{\phi}_{t+1}
\end{align}

\subsection{Gradient TD Methods}

To handle function approximation more rigorously, gradient TD methods minimize the mean squared projected Bellman error:

\begin{align}
\text{MSPBE}(\mathbf{w}) &= \|\boldsymbol{\Pi}(\mathbf{T}^\pi \hat{\mathbf{v}} - \hat{\mathbf{v}})\|_{\mathbf{D}}^2 \\
\nabla \text{MSPBE}(\mathbf{w}) &= 2\boldsymbol{\Phi}^T \mathbf{D} (\boldsymbol{\Pi}(\mathbf{T}^\pi \hat{\mathbf{v}} - \hat{\mathbf{v}}))
\end{align}

\section{Chapter Summary}

This chapter developed the mathematical foundations of temporal difference learning:

\begin{itemize}
    \item TD(0) algorithm with convergence analysis using stochastic approximation theory
    \item Bias-variance tradeoff analysis showing TD's advantage in variance reduction
    \item TD(λ) and eligibility traces providing a spectrum between TD(0) and Monte Carlo
    \item Convergence theory for linear function approximation with error bounds
    \item Comparative analysis with Monte Carlo and dynamic programming methods
    \item Advanced topics including multi-step methods and gradient TD approaches
\end{itemize}

Temporal difference learning provides the foundation for many modern RL algorithms, combining the best aspects of Monte Carlo and dynamic programming approaches. The next chapter extends these ideas to action-value methods with Q-learning and SARSA.