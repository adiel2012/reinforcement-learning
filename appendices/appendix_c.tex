\chapter{Case Studies}
\label{app:case-studies}

This appendix presents detailed case studies that demonstrate the application of reinforcement learning to real-world engineering problems. Each case study includes problem formulation, algorithm selection and tuning, implementation details, and lessons learned.

\section{Case Study 1: Autonomous Drone Navigation}

\subsection{Problem Description}

A quadrotor drone must navigate through a complex environment with obstacles while minimizing energy consumption and flight time. The drone has continuous state and action spaces, making this a challenging continuous control problem.

\textbf{State Space:} $s = (x, y, z, \dot{x}, \dot{y}, \dot{z}, \phi, \theta, \psi, \dot{\phi}, \dot{\theta}, \dot{\psi}) \in \real^{12}$

where $(x,y,z)$ is position, $(\dot{x}, \dot{y}, \dot{z})$ is velocity, $(\phi, \theta, \psi)$ are Euler angles, and $(\dot{\phi}, \dot{\theta}, \dot{\psi})$ are angular velocities.

\textbf{Action Space:} $a = (T, \tau_\phi, \tau_\theta, \tau_\psi) \in \real^4$

where $T$ is thrust and $(\tau_\phi, \tau_\theta, \tau_\psi)$ are torques about each axis.

\textbf{Dynamics:} The quadrotor dynamics are governed by:
\begin{align}
m\ddot{\mathbf{r}} &= T\mathbf{R}\mathbf{e}_3 - mg\mathbf{e}_3 \\
\mathbf{I}\dot{\boldsymbol{\omega}} &= \boldsymbol{\tau} - \boldsymbol{\omega} \times \mathbf{I}\boldsymbol{\omega}
\end{align}

where $\mathbf{R}$ is the rotation matrix and $\mathbf{I}$ is the inertia tensor.

\subsection{Algorithm Selection and Implementation}

\textbf{Algorithm Choice:} Deep Deterministic Policy Gradient (DDPG) was selected for its ability to handle continuous action spaces and its sample efficiency.

\textbf{Network Architecture:}
\begin{itemize}
    \item Actor: $[12] \to [256] \to [256] \to [4]$ with tanh output activation
    \item Critic: $[16] \to [256] \to [256] \to [1]$ (state-action input)
    \item Target networks with soft updates ($\tau = 0.001$)
\end{itemize}

\textbf{Reward Function:}
\begin{equation}
r(s,a) = -\|s_{pos} - s_{target}\|^2 - 0.1\|a\|^2 - 10 \cdot \mathbf{1}_{collision}
\end{equation}

\subsection{Training Process and Results}

\textbf{Training Configuration:}
\begin{itemize}
    \item Episodes: 2000
    \item Steps per episode: 1000
    \item Replay buffer size: $10^6$
    \item Batch size: 256
    \item Learning rates: Actor $10^{-4}$, Critic $10^{-3}$
    \item Exploration noise: Ornstein-Uhlenbeck process
\end{itemize}

\textbf{Performance Metrics:}
\begin{itemize}
    \item Success rate: 94\% (reaching target within 1m)
    \item Average flight time: 12.3 seconds
    \item Energy efficiency: 15\% improvement over PID controller
    \item Collision rate: 2\%
\end{itemize}

\subsection{Lessons Learned}

\begin{enumerate}
    \item \textbf{Reward Shaping Critical:} Initial sparse rewards led to poor exploration. Dense reward with distance-based terms significantly improved learning.
    
    \item \textbf{Curriculum Learning Effective:} Starting with simple environments and gradually increasing complexity improved sample efficiency.
    
    \item \textbf{Simulation-to-Reality Gap:} Robust training with domain randomization was essential for real-world transfer.
    
    \item \textbf{Safety Considerations:} Emergency safety controller was necessary during real-world testing.
\end{enumerate}

\section{Case Study 2: Smart Grid Energy Management}

\subsection{Problem Description}

A microgrid with renewable energy sources, battery storage, and flexible loads must optimize energy dispatch to minimize costs while maintaining reliability constraints.

\textbf{State Space:}
\begin{itemize}
    \item Battery state of charge: $SOC \in [0, 1]$
    \item Renewable generation forecast: $P_{ren} \in [0, P_{max}]$
    \item Load demand forecast: $P_{load} \in [0, P_{max}]$
    \item Electricity price: $\lambda \in [\lambda_{min}, \lambda_{max}]$
    \item Time of day: $t \in [0, 23]$
\end{itemize}

\textbf{Action Space:}
\begin{itemize}
    \item Battery charge/discharge power: $P_{bat} \in [-P_{bat,max}, P_{bat,max}]$
    \item Grid import/export: $P_{grid} \in [-P_{grid,max}, P_{grid,max}]$
    \item Load curtailment: $P_{curt} \in [0, P_{load}]$
\end{itemize}

\subsection{MDP Formulation}

\textbf{Transition Dynamics:}
\begin{align}
SOC_{t+1} &= SOC_t + \frac{\eta P_{bat,t} \Delta t}{E_{bat,max}} \\
P_{balance} &= P_{ren} + P_{grid} + P_{bat} - P_{load} + P_{curt}
\end{align}

\textbf{Constraints:}
\begin{align}
SOC_{min} \leq SOC_t &\leq SOC_{max} \\
|P_{balance}| &\leq \epsilon \quad \text{(power balance)}
\end{align}

\textbf{Reward Function:}
\begin{equation}
r_t = -(\lambda_t P_{grid,t} \Delta t + C_{curt} P_{curt,t} + C_{deg} |P_{bat,t}|)
\end{equation}

\subsection{Implementation Details}

\textbf{Algorithm:} Soft Actor-Critic (SAC) for its sample efficiency and robustness.

\textbf{Feature Engineering:}
\begin{itemize}
    \item Moving averages of renewable generation and demand
    \item Seasonal and diurnal patterns encoded as sinusoidal features
    \item Price forecasts using historical patterns
\end{itemize}

\textbf{Constraint Handling:} Projection method to ensure feasible actions:
\begin{equation}
a_{feasible} = \Pi_{\mathcal{A}}(a_{proposed})
\end{equation}

\subsection{Results and Performance Analysis}

\textbf{Performance Comparison:}
\begin{center}
\begin{tabular}{lccc}
\toprule
Method & Daily Cost (\$) & Renewable Utilization (\%) & Constraint Violations \\
\midrule
Rule-based & 142.50 & 78.3 & 0 \\
MPC & 138.20 & 82.1 & 0 \\
SAC & 134.80 & 85.7 & 0.02\% \\
\bottomrule
\end{tabular}
\end{center}

\textbf{Key Insights:}
\begin{enumerate}
    \item 5.4\% cost reduction compared to rule-based controller
    \item 2.6\% cost reduction compared to model predictive control
    \item Learned to anticipate price patterns and pre-charge batteries
    \item Robust performance under forecast uncertainties
\end{enumerate}

\section{Case Study 3: Manufacturing Process Optimization}

\subsection{Problem Description}

A chemical batch process must optimize temperature and pressure profiles to maximize product yield while minimizing energy consumption and processing time.

\textbf{Process Dynamics:}
\begin{align}
\frac{dC_A}{dt} &= -k_1(T) C_A \\
\frac{dC_B}{dt} &= k_1(T) C_A - k_2(T) C_B \\
\frac{dT}{dt} &= \frac{Q - Q_{loss}(T)}{mC_p}
\end{align}

where $k_i(T) = A_i \exp(-E_i/RT)$ are temperature-dependent rate constants.

\subsection{Multi-Objective Optimization}

\textbf{Objectives:}
\begin{align}
J_1 &= \text{maximize } C_B(t_f) \quad \text{(yield)} \\
J_2 &= \text{minimize } \int_0^{t_f} Q(t) dt \quad \text{(energy)} \\
J_3 &= \text{minimize } t_f \quad \text{(time)}
\end{align}

\textbf{Scalarization:} Weighted sum approach:
\begin{equation}
r(s,a) = w_1 \frac{C_B(t_f)}{C_{B,max}} - w_2 \frac{Q(t)}{Q_{max}} - w_3 \frac{1}{t_{max}}
\end{equation}

\subsection{Implementation and Results}

\textbf{Algorithm:} Twin Delayed DDPG (TD3) for stability in continuous control.

\textbf{Results:}
\begin{itemize}
    \item 12\% improvement in product yield
    \item 18\% reduction in energy consumption
    \item 8\% reduction in batch time
    \item Consistent performance across different initial conditions
\end{itemize}

\section{Performance Comparisons}

\subsection{Algorithm Performance Summary}

\begin{center}
\begin{tabular}{lcccc}
\toprule
Case Study & Problem Type & Algorithm & Sample Efficiency & Final Performance \\
\midrule
Drone Navigation & Continuous Control & DDPG & Medium & 94\% success \\
Smart Grid & Constrained Control & SAC & High & 5.4\% improvement \\
Manufacturing & Multi-objective & TD3 & Medium & 12\% yield gain \\
\bottomrule
\end{tabular}
\end{center}

\subsection{Common Success Factors}

\begin{enumerate}
    \item \textbf{Domain Knowledge Integration:} Incorporating engineering insights into reward design and feature engineering
    
    \item \textbf{Simulation Fidelity:} High-fidelity simulators were crucial for initial learning
    
    \item \textbf{Constraint Handling:} Explicit constraint enforcement prevented unsafe exploration
    
    \item \textbf{Robust Training:} Domain randomization and robust optimization improved real-world performance
\end{enumerate}

\section{Troubleshooting Guide}

\subsection{Common Training Issues}

\textbf{Poor Convergence:}
\begin{itemize}
    \item Check reward function scaling and normalization
    \item Verify network initialization and learning rates
    \item Ensure sufficient exploration during early training
    \item Monitor gradient norms for vanishing/exploding gradients
\end{itemize}

\textbf{Unstable Training:}
\begin{itemize}
    \item Reduce learning rates, especially for critic networks
    \item Use target networks with appropriate update rates
    \item Implement gradient clipping
    \item Check for numerical instabilities in environment dynamics
\end{itemize}

\textbf{Poor Real-World Transfer:}
\begin{itemize}
    \item Increase simulation realism and randomization
    \item Implement domain adaptation techniques
    \item Use conservative policy updates
    \item Include safety constraints and emergency controllers
\end{itemize}

\subsection{Hyperparameter Tuning Guidelines}

\textbf{Learning Rates:}
\begin{itemize}
    \item Actor: typically $10^{-4}$ to $10^{-3}$
    \item Critic: typically $10^{-3}$ to $10^{-2}$
    \item Use learning rate schedules for long training runs
\end{itemize}

\textbf{Network Architecture:}
\begin{itemize}
    \item Start with 2-3 hidden layers of 256-512 units
    \item Use batch normalization for deep networks
    \item Consider layer normalization for recurrent policies
\end{itemize}

\textbf{Exploration:}
\begin{itemize}
    \item Gaussian noise: $\sigma = 0.1$ to $0.2$ of action range
    \item Ornstein-Uhlenbeck: $\theta = 0.15$, $\sigma = 0.2$
    \item Decay exploration over training
\end{itemize}

\subsection{Debugging Checklist}

\begin{enumerate}
    \item \textbf{Environment Sanity Checks:}
    \begin{itemize}
        \item Verify state and action space definitions
        \item Test random policy performance
        \item Check reward function computation
        \item Validate episode termination conditions
    \end{itemize}
    
    \item \textbf{Algorithm Implementation:}
    \begin{itemize}
        \item Verify gradient computation and backpropagation
        \item Check replay buffer implementation
        \item Validate target network updates
        \item Monitor loss functions and metrics
    \end{itemize}
    
    \item \textbf{Training Diagnostics:}
    \begin{itemize}
        \item Plot learning curves and moving averages
        \item Monitor exploration statistics
        \item Track gradient norms and weight distributions
        \item Analyze action distributions over time
    \end{itemize}
\end{enumerate}