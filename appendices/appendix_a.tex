\chapter{Mathematical Reference}
\label{app:math-reference}

This appendix provides a comprehensive mathematical reference for concepts used throughout the book. It serves as a quick reference for key mathematical tools and theorems.

\section{Matrix Calculus for RL}

\subsection{Gradients and Jacobians}

For scalar function $f: \real^n \to \real$, the gradient is:
\begin{equation}
\nabla f(x) = \begin{bmatrix} \frac{\partial f}{\partial x_1} \\ \vdots \\ \frac{\partial f}{\partial x_n} \end{bmatrix}
\end{equation}

For vector function $F: \real^n \to \real^m$, the Jacobian is:
\begin{equation}
J_F(x) = \begin{bmatrix}
\frac{\partial F_1}{\partial x_1} & \cdots & \frac{\partial F_1}{\partial x_n} \\
\vdots & \ddots & \vdots \\
\frac{\partial F_m}{\partial x_1} & \cdots & \frac{\partial F_m}{\partial x_n}
\end{bmatrix}
\end{equation}

\subsection{Chain Rule}

For composite functions $h(x) = f(g(x))$:
\begin{equation}
\nabla h(x) = J_g(x)^T \nabla f(g(x))
\end{equation}

\subsection{Common Derivatives}

\begin{align}
\frac{\partial}{\partial x} x^T A x &= (A + A^T) x \\
\frac{\partial}{\partial x} a^T x &= a \\
\frac{\partial}{\partial X} \text{tr}(AXB) &= A^T B^T \\
\frac{\partial}{\partial X} \log \det(X) &= (X^{-1})^T
\end{align}

\section{Probability Distributions Commonly Used}

\subsection{Discrete Distributions}

\textbf{Bernoulli Distribution:} $X \sim \text{Bernoulli}(p)$
\begin{align}
P(X = 1) &= p, \quad P(X = 0) = 1-p \\
\expect[X] &= p, \quad \text{Var}(X) = p(1-p)
\end{align}

\textbf{Categorical Distribution:} $X \sim \text{Categorical}(\mathbf{p})$
\begin{align}
P(X = k) &= p_k, \quad \sum_{k=1}^K p_k = 1 \\
\expect[X] &= \sum_{k=1}^K k p_k
\end{align}

\subsection{Continuous Distributions}

\textbf{Normal Distribution:} $X \sim \mathcal{N}(\mu, \sigma^2)$
\begin{align}
f(x) &= \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right) \\
\expect[X] &= \mu, \quad \text{Var}(X) = \sigma^2
\end{align}

\textbf{Multivariate Normal:} $\mathbf{X} \sim \mathcal{N}(\boldsymbol{\mu}, \Sigma)$
\begin{align}
f(\mathbf{x}) &= \frac{1}{(2\pi)^{k/2}|\Sigma|^{1/2}} \exp\left(-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^T \Sigma^{-1} (\mathbf{x}-\boldsymbol{\mu})\right) \\
\expect[\mathbf{X}] &= \boldsymbol{\mu}, \quad \text{Cov}(\mathbf{X}) = \Sigma
\end{align}

\section{Optimization Algorithms Summary}

\subsection{Gradient Descent Variants}

\textbf{Vanilla Gradient Descent:}
\begin{equation}
\theta_{t+1} = \theta_t - \alpha \nabla f(\theta_t)
\end{equation}

\textbf{Momentum:}
\begin{align}
v_{t+1} &= \beta v_t + \nabla f(\theta_t) \\
\theta_{t+1} &= \theta_t - \alpha v_{t+1}
\end{align}

\textbf{Adam:}
\begin{align}
m_t &= \beta_1 m_{t-1} + (1-\beta_1) \nabla f(\theta_t) \\
v_t &= \beta_2 v_{t-1} + (1-\beta_2) [\nabla f(\theta_t)]^2 \\
\hat{m}_t &= \frac{m_t}{1-\beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1-\beta_2^t} \\
\theta_{t+1} &= \theta_t - \alpha \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
\end{align}

\section{Convergence Analysis Techniques}

\subsection{Lyapunov Functions}

A function $V: \mathcal{X} \to \real_+$ is a Lyapunov function for dynamical system $x_{t+1} = f(x_t)$ if:
\begin{enumerate}
    \item $V(x) > 0$ for $x \neq x^*$ and $V(x^*) = 0$
    \item $V(f(x)) - V(x) \leq 0$ for all $x$
\end{enumerate}

\subsection{Martingale Convergence Theorem}

\begin{theorem}[Martingale Convergence]
Let $\{X_t\}$ be a supermartingale that is bounded below. Then $X_t$ converges almost surely to a finite random variable.
\end{theorem}

\subsection{Robbins-Monro Conditions}

For stochastic approximation algorithm $\theta_{t+1} = \theta_t + \alpha_t H(\theta_t, \xi_t)$, convergence occurs under:
\begin{align}
\sum_{t=0}^\infty \alpha_t &= \infty \\
\sum_{t=0}^\infty \alpha_t^2 &< \infty \\
\expect[H(\theta, \xi)] &= h(\theta) \text{ has unique zero at } \theta^*
\end{align}